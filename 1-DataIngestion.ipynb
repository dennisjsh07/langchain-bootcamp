{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07131530",
   "metadata": {},
   "source": [
    "### Data Ingestion - Document loaders\n",
    "\n",
    "https://docs.langchain.com/oss/python/integrations/document_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4fbf6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='Freedom was not gifted; it was earned through courage and sacrifice.\\nCountless voices rose together to demand dignity and self-rule.\\nEvery step toward independence carried the weight of hope and loss.\\nThe struggle taught us unity beyond language, region, or belief.\\nIndependence is not just a date, but a responsibility we carry daily.\\nIt reminds us to protect justice, equality, and truth.\\nThe past whispers lessons of resilience and bravery.\\nThe present asks us to build with integrity and compassion.\\nThe future depends on how wisely we use our freedom today.\\nIndependence lives on when we choose progress over fear.\\n')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Text Loader for .txt files\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"speech.txt\")\n",
    "docs = loader.load()\n",
    "docs\n",
    "# print(type(docs[0]))  # langchain.document.Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35bfab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m118', 'creator': 'Chromium', 'creationdate': '2025-12-04T07:07:18+00:00', 'moddate': '2025-12-04T07:07:18+00:00', 'source': 'samplePdf.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content=\"Billable152\\x0000hNon Billable80\\x0000hTotal232\\x0000h\\nYasref SCH\\nImplementation\\n04\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring and fixing bugs in pages,\\nbv's, and ruleset's required for\\nvendor registration and modification\\nprocess.f\\nDJDennis Joshua\\nYasref SCH\\nImplementation\\n04\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring and fixing bugs in pages,\\nbv's, and ruleset's required for\\nvendor registration and modification\\nprocess.\\nDJDennis Joshua\\nHolidays and\\nLeave [Only]\\n08\\x0000 DJDennis Joshua - 5000059 Non\\nBillable Approved satuday. DJDennis Joshua\\nHolidays and\\nLeave [Only]\\n08\\x0000 DJDennis Joshua - 5000059 Non\\nBillable Approved Friday. DJDennis Joshua\\nYasref SCH\\nImplementation\\n08\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring roles, fixing bugs in\\npages and bv's for the\\nimplementation of permission\\nengine.\\nDJDennis Joshua\\nYasref SCH\\nImplementation\\n04\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nbug fixing and configuring pages,\\nbv's and rulesets required for\\nregistration and modification\\nprocess.\\nDJDennis Joshua\\nYasref SCH\\nImplementation\\n04\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring pages, bv's, ruleset's and\\nworkflow's for registration,\\nmodification flows.\\nDJDennis Joshua\\n08\\x000008\\x000000\\x0000\\n08\\x000000\\x000008\\x0000\\n08\\x000000\\x000008\\x0000\\n08\\x000008\\x000000\\x0000\\n04\\x000004\\x000000\\x0000\\n04\\x000004\\x000000\\x0000\\n08\\x000008\\x000000\\x0000\\nYSR-T760 UT, SIT - QA\\ue9b2\\nYSR-T743 UT, SIT - QA\\ue9b2\\nHLB-T5 Holiday - Weekend\\ue9b2\\nHLB-T5 Holiday - Weekend\\ue9b2\\nYSR-T1300 UT, SIT - QA\\ue9b2\\nYSR-T760 UT, SIT - QA\\ue9b2\\nYSR-T1220 UT, SIT - QA\\ue9b2\\n\\uedfd 30-11-2025\\ue94d\\n\\uedfd 29-11-2025\\ue94d\\n\\uedfd 28-11-2025\\ue94d\\n\\uedfd 27-11-2025\\ue94d\\n\\uedfd 26-11-2025\\ue94d\\n\\uedfd 25-11-2025\\ue94d\\n\\uedfd 24-11-2025\\ue94d\\n\\ue9b1Project \\ue92cDaily Log Hours \\ue92cTime Period \\uea89User \\uea21Billing…\\uea21Approval Status \\uea1eNotes \\uea89Created By\\ueb1a ID Log Title\\nSupply Chain Hub\\nTimesheet List view\\nProject Name:All Projects View:Group By Date Custom View Name:My Time Logs Date:01-11-2025 to 30-11-2025 Exported On:04-12-2025 12\\x0037 PM\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m118', 'creator': 'Chromium', 'creationdate': '2025-12-04T07:07:18+00:00', 'moddate': '2025-12-04T07:07:18+00:00', 'source': 'samplePdf.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content=\"Yasref SCH\\nImplementation\\n08\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring pages, bv's, ruleset's\\nrequired for registration,\\nmodification etc.\\nDJDennis Joshua\\nHolidays and\\nLeave [Only]\\n08\\x0000 DJDennis Joshua - 5000059 Non\\nBillable Approved casual leave DJDennis Joshua\\nHolidays and\\nLeave [Only]\\n08\\x0000 DJDennis Joshua - 5000059 Non\\nBillable Approved saturday DJDennis Joshua\\nYasref SCH\\nImplementation\\n04\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring and testing permission\\nengine flow for pages, bv's and\\nrulesets.\\nDJDennis Joshua\\nYasref SCH\\nImplementation\\n04\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring and testing pages, bv's,\\nrulesets and workflow's for\\nregistration and modification\\nprocess.\\nDJDennis Joshua\\nYasref SCH\\nImplementation\\n04\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring and testing permission\\nengine flow for pages, bv's and\\nrulesets.\\nDJDennis Joshua\\nYasref SCH\\nImplementation\\n04\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring and testing pages, bv's,\\nrulesets and workflow's for\\nregistration and modification\\nprocess.\\nDJDennis Joshua\\nYasref SCH\\nImplementation\\n04\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring and testing permission\\nengine flow for pages, bv's and\\nrulesets.\\nDJDennis Joshua\\nYasref SCH\\nImplementation\\n04\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring and testing pages, bv's,\\nrulesets and workflow's for\\nregistration and modification\\nprocess.\\nDJDennis Joshua\\n08\\x000000\\x000008\\x0000\\n08\\x000000\\x000008\\x0000\\n08\\x000008\\x000000\\x0000\\n08\\x000008\\x000000\\x0000\\n08\\x000008\\x000000\\x0000\\nYSR-T743 UT, SIT - QA\\ue9b2\\nHLB-T7 Leave (Personal)\\ue9b2\\nHLB-T5 Holiday - Weekend\\ue9b2\\nYSR-T1299 UT, SIT - Dev\\ue9b2\\nYSR-T1220 UT, SIT - QA\\ue9b2\\nYSR-T1299 UT, SIT - Dev\\ue9b2\\nYSR-T1220 UT, SIT - QA\\ue9b2\\nYSR-T1299 UT, SIT - Dev\\ue9b2\\nYSR-T760 UT, SIT - QA\\ue9b2\\n\\uedfd 23-11-2025\\ue94d\\n\\uedfd 22-11-2025\\ue94d\\n\\uedfd 21-11-2025\\ue94d\\n\\uedfd 20-11-2025\\ue94d\\n\\uedfd 19-11-2025\\ue94d\\n\\ue9b1Project \\ue92cDaily Log Hours \\ue92cTime Period \\uea89User \\uea21Billing…\\uea21Approval Status \\uea1eNotes \\uea89Created By\\ueb1a ID Log Title\\nSupply Chain Hub\\nTimesheet List view\\nProject Name:All Projects View:Group By Date Custom View Name:My Time Logs Date:01-11-2025 to 30-11-2025 Exported On:04-12-2025 12\\x0037 PM\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m118', 'creator': 'Chromium', 'creationdate': '2025-12-04T07:07:18+00:00', 'moddate': '2025-12-04T07:07:18+00:00', 'source': 'samplePdf.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content=\"Yasref SCH\\nImplementation\\n04\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring and testing permission\\nengine flow for pages, bv's and\\nrulesets.\\nDJDennis Joshua\\nYasref SCH\\nImplementation\\n04\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring and testing pages, bv's,\\nrulesets and workflow's for\\nregistration and modification\\nprocess.\\nDJDennis Joshua\\nYasref SCH\\nImplementation\\n04\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring and testing pages,\\nbusinessview's , rulesets and\\nworklfows for registration and\\nmodification process\\nDJDennis Joshua\\nYasref SCH\\nImplementation\\n04\\x0000 DJDennis Joshua - 5000059 Billable Approved configuring and testing the new\\npermission engine configuration.\\nDJDennis Joshua\\nYasref SCH\\nImplementation\\n04\\x0000 DJDennis Joshua - 5000059 Billable Approved configuring and testing the new\\npermission engine configuration.\\nDJDennis Joshua\\nYasref SCH\\nImplementation\\n02\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring and fixing bugs in page,\\nbv's , rulesets and workflow's for\\nregistration and modification\\nprocess.\\nDJDennis Joshua\\nYasref SCH\\nImplementation\\n02\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring and fixing bugs in page,\\nbv's , rulesets and workflow's for\\nregistration and modification\\nprocess.\\nDJDennis Joshua\\nHolidays and\\nLeave [Only]\\n08\\x0000 DJDennis Joshua - 5000059 Non\\nBillable Approved saturday DJDennis Joshua\\nHolidays and\\nLeave [Only]\\n08\\x0000 DJDennis Joshua - 5000059 Non\\nBillable Approved Friday. DJDennis Joshua\\n08\\x000008\\x000000\\x0000\\n08\\x000008\\x000000\\x0000\\n08\\x000008\\x000000\\x0000\\n08\\x000000\\x000008\\x0000\\n08\\x000000\\x000008\\x0000\\nYSR-T1299 UT, SIT - Dev\\ue9b2\\nYSR-T743 UT, SIT - QA\\ue9b2\\nYSR-T743 UT, SIT - QA\\ue9b2\\nYSR-T1220 UT, SIT - QA\\ue9b2\\nYSR-T1220 UT, SIT - QA\\ue9b2\\nYSR-T760 UT, SIT - QA\\ue9b2\\nYSR-T743 UT, SIT - QA\\ue9b2\\nHLB-T5 Holiday - Weekend\\ue9b2\\nHLB-T5 Holiday - Weekend\\ue9b2\\n\\uedfd 18-11-2025\\ue94d\\n\\uedfd 17-11-2025\\ue94d\\n\\uedfd 16-11-2025\\ue94d\\n\\uedfd 15-11-2025\\ue94d\\n\\uedfd 14-11-2025\\ue94d\\n\\ue9b1Project \\ue92cDaily Log Hours \\ue92cTime Period \\uea89User \\uea21Billing…\\uea21Approval Status \\uea1eNotes \\uea89Created By\\ueb1a ID Log Title\\nSupply Chain Hub\\nTimesheet List view\\nProject Name:All Projects View:Group By Date Custom View Name:My Time Logs Date:01-11-2025 to 30-11-2025 Exported On:04-12-2025 12\\x0037 PM\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m118', 'creator': 'Chromium', 'creationdate': '2025-12-04T07:07:18+00:00', 'moddate': '2025-12-04T07:07:18+00:00', 'source': 'samplePdf.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content=\"Yasref SCH\\nImplementation\\n08\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring pages, bv's, rulesets,\\nworkflow's required for registration\\nand modification process.\\nDJDennis Joshua\\nYasref SCH\\nImplementation\\n08\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring pages, bv's, rulesets,\\nworkflow's required for registration\\nand modification process.\\nDJDennis Joshua\\nHolidays and\\nLeave [Only]\\n08\\x0000 DJDennis Joshua - 5000059 Non\\nBillable Approved sick leave DJDennis Joshua\\nYasref SCH\\nImplementation\\n08\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring pages, bv's, rulesets,\\nworkflow's required for registration\\nand modification process.\\nDJDennis Joshua\\nYasref SCH\\nImplementation\\n08\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring pages, bv's, rulesets,\\nworkflow's required for registration\\nand modification process.\\nDJDennis Joshua\\nHolidays and\\nLeave [Only]\\n08\\x0000 DJDennis Joshua - 5000059 Non\\nBillable\\nApproved saturday DJDennis Joshua\\nHolidays and\\nLeave [Only]\\n08\\x0000 DJDennis Joshua - 5000059 Non\\nBillable\\nApproved friday DJDennis Joshua\\nYasref SCH\\nImplementation\\n08\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring pages, bv's, rulesets and\\nintegrations for registration and\\nonboarding process.\\nDJDennis Joshua\\n08\\x000008\\x000000\\x0000\\n08\\x000008\\x000000\\x0000\\n08\\x000000\\x000008\\x0000\\n08\\x000008\\x000000\\x0000\\n08\\x000008\\x000000\\x0000\\n08\\x000000\\x000008\\x0000\\n08\\x000000\\x000008\\x0000\\n08\\x000008\\x000000\\x0000\\n08\\x000008\\x000000\\x0000\\nYSR-T760 UT, SIT - QA\\ue9b2\\nYSR-T760 UT, SIT - QA\\ue9b2\\nHLB-T7 Leave (Personal)\\ue9b2\\nYSR-T743 UT, SIT - QA\\ue9b2\\nYSR-T743 UT, SIT - QA\\ue9b2\\nHLB-T5 Holiday - Weekend\\ue9b2\\nHLB-T5 Holiday - Weekend\\ue9b2\\nYSR-T1220 UT, SIT - QA\\ue9b2\\n\\uedfd 13-11-2025\\ue94d\\n\\uedfd 12-11-2025\\ue94d\\n\\uedfd 11-11-2025\\ue94d\\n\\uedfd 10-11-2025\\ue94d\\n\\uedfd 09-11-2025\\ue94d\\n\\uedfd 08-11-2025\\ue94d\\n\\uedfd 07-11-2025\\ue94d\\n\\uedfd 06-11-2025\\ue94d\\n\\uedfd 05-11-2025\\ue94d\\n\\ue9b1Project \\ue92cDaily Log Hours \\ue92cTime Period \\uea89User \\uea21Billing…\\uea21Approval Status \\uea1eNotes \\uea89Created By\\ueb1a ID Log Title\\nSupply Chain Hub\\nTimesheet List view\\nProject Name:All Projects View:Group By Date Custom View Name:My Time Logs Date:01-11-2025 to 30-11-2025 Exported On:04-12-2025 12\\x0037 PM\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m118', 'creator': 'Chromium', 'creationdate': '2025-12-04T07:07:18+00:00', 'moddate': '2025-12-04T07:07:18+00:00', 'source': 'samplePdf.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content=\"Yasref SCH\\nImplementation\\n08\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring pages, bv's, rulesets and\\nintegrations for registration and\\nonboarding process.\\nDJDennis Joshua\\nYasref SCH\\nImplementation\\n08\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring pages, bv's, rulesets and\\nintegrations for registration and\\nonboarding process.\\nDJDennis Joshua\\nYasref SCH\\nImplementation\\n08\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring pages, bv's, rulesets and\\nintegrations for registration and\\nonboarding process.\\nDJDennis Joshua\\nYasref SCH\\nImplementation\\n08\\x0000 DJDennis Joshua - 5000059 Billable\\nApproved\\nconfiguring pages, bv's, rulesets and\\nintegrations for registration and\\nonboarding process.\\nDJDennis Joshua\\nHolidays and\\nLeave [Only]\\n08\\x0000 DJDennis Joshua - 5000059 Non\\nBillable Approved saturday. DJDennis Joshua\\n08\\x000008\\x000000\\x0000\\n08\\x000008\\x000000\\x0000\\n08\\x000008\\x000000\\x0000\\n08\\x000000\\x000008\\x0000\\nYSR-T760 UT, SIT - QA\\ue9b2\\nYSR-T760 UT, SIT - QA\\ue9b2\\nYSR-T743 UT, SIT - QA\\ue9b2\\nYSR-T743 UT, SIT - QA\\ue9b2\\nHLB-T5 Holiday - Weekend\\ue9b2\\n\\uedfd 04-11-2025\\ue94d\\n\\uedfd 03-11-2025\\ue94d\\n\\uedfd 02-11-2025\\ue94d\\n\\uedfd 01-11-2025\\ue94d\\n\\ue9b1Project \\ue92cDaily Log Hours \\ue92cTime Period \\uea89User \\uea21Billing…\\uea21Approval Status \\uea1eNotes \\uea89Created By\\ueb1a ID Log Title\\nSupply Chain Hub\\nTimesheet List view\\nProject Name:All Projects View:Group By Date Custom View Name:My Time Logs Date:01-11-2025 to 30-11-2025 Exported On:04-12-2025 12\\x0037 PM\")]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading PDF using pyPDF\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"samplePdf.pdf\")\n",
    "docs = loader.load()\n",
    "docs\n",
    "# print(type(docs[0]))  # langchain.document.Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40677bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\\n      LLM Powered Autonomous Agents\\n    Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\n\\nExamples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\n\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\n\\nIllustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\n\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\n\\nExperiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\n\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\n\\nAfter fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\n\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\n\\nIllustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\n\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\n\\nComparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\n\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\n\\nComparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\n\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\n\\nA picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\n\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\n\\nIllustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\n\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\n\\nPseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\n\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe generative agent architecture. (Image source: Park et al. 2023)\\n\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## web base loader\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_path=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\"),\n",
    "    bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"post-title\", \"post-content\"))),\n",
    ")\n",
    "docs = loader.load()\n",
    "docs\n",
    "# print(type(docs[0]))  # langchain.document.Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa41e225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2025-09-29', 'Title': 'Efficient Reasoning Models: A Survey', 'Authors': 'Sicheng Feng, Gongfan Fang, Xinyin Ma, Xinchao Wang', 'Summary': 'Reasoning models have demonstrated remarkable progress in solving complex and logic-intensive tasks by generating extended Chain-of-Thoughts (CoTs) prior to arriving at a final answer. Yet, the emergence of this \"slow-thinking\" paradigm, with numerous tokens generated in sequence, inevitably introduces substantial computational overhead. To this end, it highlights an urgent need for effective acceleration. This survey aims to provide a comprehensive overview of recent advances in efficient reasoning. It categorizes existing works into three key directions: (1) shorter - compressing lengthy CoTs into concise yet effective reasoning chains; (2) smaller - developing compact language models with strong reasoning capabilities through techniques such as knowledge distillation, other model compression techniques, and reinforcement learning; and (3) faster - designing efficient decoding strategies to accelerate inference of reasoning models. A curated collection of papers discussed in this survey is available in our GitHub repository: https://github.com/fscdc/Awesome-Efficient-Reasoning-Models.'}, page_content='Published in Transactions on Machine Learning Research (09/2025)\\nEfficient Reasoning Models: A Survey\\nSicheng Feng\\nsicheng@mail.nankai.edu.cn\\nNational University of Singapore, Singapore\\nNankai University, Tianjin, China\\nGongfan Fang\\ngongfan@u.nus.edu\\nNational University of Singapore, Singapore\\nXinyin Ma\\nmaxinyin@u.nus.edu\\nNational University of Singapore, Singapore\\nXinchao Wang∗\\nxinchao@nus.edu.sg\\nNational University of Singapore, Singapore\\nReviewed on OpenReview: https: // openreview. net/ forum? id= sySqlxj8EB\\nAbstract\\nReasoning models have demonstrated remarkable progress in solving complex and logic-\\nintensive tasks by generating extended Chain-of-Thoughts (CoTs) prior to arriving at a\\nfinal answer.\\nYet, the emergence of this “slow-thinking” paradigm, with numerous to-\\nkens generated in sequence, inevitably introduces substantial computational overhead. To\\nthis end, it highlights an urgent need for effective acceleration. This survey aims to pro-\\nvide a comprehensive overview of recent advances in efficient reasoning.\\nIt categorizes\\nexisting works into three key directions:\\n(1) shorter – compressing lengthy CoTs into\\nconcise yet effective reasoning chains; (2) smaller – developing compact language mod-\\nels with strong reasoning capabilities through techniques such as knowledge distillation,\\nother model compression techniques, and reinforcement learning; and (3) faster – de-\\nsigning efficient decoding strategies to accelerate inference of reasoning models.\\nA cu-\\nrated collection of papers discussed in this survey is available in our GitHub repository:\\nhttps://github.com/fscdc/Awesome-Efficient-Reasoning-Models.\\n1\\nIntroduction\\nRecent reasoning-oriented models, or Large Reasoning Models (LRMs) (Guo et al., 2025; Jaech et al., 2024),\\nhave achieved remarkable performance on complex reasoning tasks by generating long Chain-of-Thoughts\\n(CoTs), enabling effective problem-solving in domains such as mathematics and coding (Sprague et al., 2024).\\nHowever, while LRMs significantly improve performance on reasoning tasks, they also cause substantial\\noverhead. Compared to standard Large Language Models (LLMs), reasoning models lead to redundancy\\nacross multiple dimensions.\\nA salient characteristic of reasoning models is their tendency to overthink by generating excessively long\\nreasoning chains (Chen et al., 2024c; Sui et al., 2025a), which has naturally motivated efforts to improve\\nefficiency by shortening reasoning paths. Meanwhile, recent studies (Wu et al., 2025d; Yang et al., 2025c;\\nJin et al., 2024b) challenge the assumption that longer CoTs always lead to better performance, showing\\neven negative returns.\\nTo address this kind of CoT length redundancy, a range of methods have been\\nproposed: reinforcement learning (RL) with length penalty (Luo et al., 2025a; Aggarwal & Welleck, 2025),\\nsupervised fine-tuning (SFT) on variable-length CoT data (Ma et al., 2025; Xia et al., 2025), and prompt-\\ndriven strategies that either guide reasoning paths or route inputs to more efficient solutions (Ding et al., 2024;\\n∗Corresponding author\\n1\\narXiv:2504.10903v2  [cs.CL]  29 Sep 2025\\nPublished in Transactions on Machine Learning Research (09/2025)\\nReasoning Ability\\nEfficient Reasoning\\nShorter\\nSmaller\\nFaster\\nPrompt-driven\\nRL\\nSFT\\nMake Long CoT Short\\nBuild Small Language Model with Strong Reasoning Ability\\nLet Decoding More Efficient\\nSLM + RL\\nDistillation\\n- Reward Model\\n- Confidence\\n- ...\\nAnswer n\\nAnswer 2\\nAnswer 1\\nstop generation\\nSampling\\nEfficient TTS Strategy\\n...\\nLatent Reasoning\\nSub-steps\\nDecomposition\\nOther Methods\\nQuantization & Pruning\\nFigure 1: Overview of efficient reasoning. We categorize existing efficient reasoning methods into three key\\ndirections based on how they improve reasoning efficiency: (1) make long CoT short (shorter); (2) build\\nsmall language models with strong reasoning ability (smaller); and (3) let decoding more efficient (faster).\\nAytes et al., 2025). Furthermore, latent reasoning performs the process in latent space without generating\\nexplicit CoTs, making reasoning chains more concise (Hao et al., 2024; Su et al., 2025).\\nIn addition to excessively long reasoning chains, reasoning models typically rely on large model sizes to\\nachieve strong reasoning performance (e.g., DeepSeek R1 (Guo et al., 2025) has 685B parameters), which\\nleads to substantial computational and memory costs.\\nTo address this, model compression (Han et al.,\\n2016) has proven effective in reducing model size redundancy in standard LLMs, naturally inspiring interest\\nin how these techniques (e.g., distillation (Hinton et al., 2015), quantization (Gray & Neuhoff, 1998), and\\npruning (LeCun et al., 1989)) can be applied to improve reasoning efficiency. In parallel, another line of\\nwork directly builds small language models with strong reasoning abilities using RL (Li et al., 2023a; 2025e;\\nZhu et al., 2024b).\\nBeyond length and model size redundancy, inefficiency can also arise during the decoding stage. A growing\\nbody of work focuses on accelerating inference through more efficient decoding strategies to tackle this\\nissue. Test-time scaling (TTS) strategies, while enhancing reasoning performance (Snell et al., 2024), also\\nintroduce latency redundancy during the decoding stage. Some methods (Sun et al., 2024a; Wang et al.,\\n2024b) specifically target and optimize the speed of certain TTS strategies (Wang et al., 2022a). Other\\napproaches, like parallel decoding (Ning et al., 2023) and problem decomposition (Teng et al., 2025), also\\nmitigate inefficiency.\\nThis survey aims to provide an overview of research in efficient reasoning.\\nAs illustrated in Figure 1,\\nwe categorize existing works into three key directions based on the type of redundancy they target: (1)\\nmaking long CoT short (shorter), which focuses on enabling models to produce shorter reasoning paths\\nwhile maintaining performance; (2) building small language model with strong reasoning abilities (smaller),\\nwhich aims to endow compact models with the ability to solve complex reasoning tasks; (3) making decoding\\nmore efficient (faster), which explores strategies to reduce latency during the decoding stage.\\nThe following sections of this survey cover the content as outlined below. Section 2 will explore key back-\\ngrounds closely related to efficient reasoning. Section 3 will systematically introduce various methods and\\ntheir relationships across three categories. Section 4 presents the evaluation metrics, as well as datasets\\nand benchmarks. Section 5 will discuss the key challenges in the field and propose some potential future\\nresearch directions, while Section 6 will conclude the survey. Additionally, Figure 2 illustrates the taxonomy\\nof efficient reasoning methods discussed in this survey.\\n2\\nPublished in Transactions on Machine Learning Research (09/2025)\\nEfficient Reasoning\\nMake Long CoT\\nShort (Shorter)\\n(§3.1)\\nRL-based\\n(§3.1.1)\\nL1 (Aggarwal & Welleck, 2025), O1-Pruner (Luo et al., 2025a), DAST (Shen et al., 2025b),\\nTHINKPRUNE (Hou et al., 2025), Kimi k1.5 (Team et al., 2025),\\nAGPO (Li et al., 2025a), Think When You Need (Yang et al., 2025b)\\nSFT-based\\n(§3.1.2)\\nTokenSkip (Xia et al., 2025), Distill2-to-1 (Yu et al., 2024), C3oT (Kang et al., 2024),\\nSPIRIT (Cui et al., 2025), TALE (Han et al., 2024), TOPS (Yang et al., 2025c),\\nCoT-Valve (Ma et al., 2025), LLM-Skip (Liu et al., 2024b), Self-training (Munkhbat et al., 2025)\\nPrompt-driven\\n(§3.1.3)\\nPrompt-guided\\nConcise CoT (Renze & Guven, 2024), MARP (Chen et al., 2024a), CoD (Xu et al., 2025c),\\nBreak the Chain (Ding et al., 2024), Token-Complexity (Lee et al., 2025)\\nPrompt\\nattribute-aware\\nClaude 3.7 Sonnet (Anthropic., 2025), RouteLLM (Ong et al., 2024),\\nSketch-of-Thought (Aytes et al., 2025), Self-REF (Chuang et al., 2024),\\nConfident or Seek Stronger (Chuang et al., 2025)\\nLatent Reasoning\\n(§3.1.4)\\nExplicit CoT to\\nImplicit CoT\\nImplicit-KD (Deng et al., 2023), Distill2-to-1 (Yu et al., 2024), CODI (Shen et al., 2025c),\\nLightThinker (Zhang et al., 2025a), CCoT (Cheng & Van Durme, 2024),\\nHCoT (Liu et al., 2024c), SoftCoT (Xu et al., 2025d), SI (Deng et al., 2024),\\nRELAY (Yu et al., 2025a), Reasoning with Latent Thoughts (Saunshi et al., 2025)\\nLatent Space\\nRepresentations\\nImplicit-KD (Deng et al., 2023), Distill2-to-1 (Yu et al., 2024), CODI (Shen et al., 2025c),\\nLightThinker (Zhang et al., 2025a), CCoT (Cheng & Van Durme, 2024),\\nHCoT (Liu et al., 2024c), SoftCoT (Xu et al., 2025d), SI (Deng et al., 2024),\\nRELAY (Yu et al., 2025a), Reasoning with Latent Thoughts (Saunshi et al., 2025),\\nPlanning-Token (Wang et al., 2024c), Filler-Token (Pfau et al., 2024),\\nCoconut (Hao et al., 2024), Heima (Shen et al., 2025a),\\nToken Assorted (Su et al., 2025), Disentangling-Memory-and-Reasoning (Jin et al., 2024a)\\nBuild Small\\nLanguage Model\\nwith Strong\\nReasoning Ability\\n(Smaller) (§3.2)\\nDistillation-based\\n(§3.2.1)\\nCoT-KD (Magister et al., 2022), MD (Li et al., 2023a), CD (Feng et al., 2024c), Mix (Li et al., 2025e),\\nNAT (Li et al., 2024a), SCORE (Zhang et al., 2024), FDD (Zhu et al., 2024b),\\nSKIntern (Liao et al., 2025b), PRR (Zhao et al., 2024), ATM (Chen et al., 2024b),\\nThinking slow, fast (Paliotta et al., 2025), Distill2-to-1 (Yu et al., 2024), DLCoT (Luo et al., 2025c),\\nM1 (Wang et al., 2025b),\\nQuantization &\\nPruning (§3.2.2)\\nSLM-Foresee (Srivastava et al., 2025), Quantization Hurts Reasoning? (Liu et al., 2025c),\\nWhen Reasoning Meets Compression (Zhang et al., 2025b)\\nRL-based\\n(§3.2.3)\\nOpen-RS (Dang & Ngo, 2025), SimpleRL-Zoo (Zeng et al., 2025a), DeepScaleR (Luo et al., 2025b)\\nLet Decoding\\nMore Efficient\\n(Faster) (§3.3)\\nEfficiency for\\nTTS strategy\\n(§3.3.1)\\nEfficient Sampling\\nFast Best-of-N (Sun et al., 2024a), ST-BoN (Wang et al., 2025d), FastMCTS (Li et al., 2025b),\\nPredictive-Decoding (Ma et al., 2024), ϕ-Decoding (Xu et al., 2025a),\\nDPTS (Ding et al., 2025), FETCH (Wang et al., 2025a)\\nEfficient\\nSelf-consistency\\nASC (Aggarwal et al., 2023), ESC (Li et al., 2024b), DSC (Wang et al., 2024b),\\nRASC (Wan et al., 2024), RPC (Zhou et al., 2025), CISC (Taubenfeld et al., 2025),\\nSelf-Calibration (Huang et al., 2025), Path-Consistency (Zhu et al., 2024a)\\nOther Methods\\n(§3.3.2)\\nAoT (Teng et al., 2025), DISC (Light et al., 2025), Meta-Reasoner (Sui et al., 2025c), AR (Liu et al., 2025b),\\nMRT (Qu et al., 2025b), TTC-Optimal Scaling (Snell et al., 2024), RSD (Liao et al., 2025a),\\nSpecReason (Pan et al., 2025), SoT (Ning et al., 2023), SGD (Jin et al., 2024c)\\nFigure 2: Taxonomy of efficient reasoning.\\n2\\nBackground\\n2.1\\nChain-of-Thought Reasoning\\nCoT (Wei et al., 2022) serves as a baseline reasoning approach, enabling LLMs to generate a sequence of\\nintermediate steps before reaching the final answer, thus significantly improving performance on complex\\nreasoning tasks. Various extensions have subsequently been proposed to further enhance reasoning capa-\\nbilities. For instance, Tree-of-Thought (ToT) (Yao et al., 2023) generalizes the linear CoT structure into\\na tree, facilitating the exploration of multiple reasoning paths through backtracking and lookahead strate-\\ngies. Graph-of-Thoughts (GoT) (Besta et al., 2024) has expanded this approach into graph structures to\\nbetter capture dependencies and compositional relationships among reasoning steps, substantially improving\\nreasoning quality. Additionally, some specialized CoT variants are task-specific. PoT (Chen et al., 2022)\\ndisentangles reasoning from computation by having the language model generate programmatic reasoning\\nsteps (i.e., expressing thoughts as code), which an external calculator executes to obtain the final answer,\\nmaking this approach particularly effective for math and financial tasks. CoS (Hu et al., 2024), on the other\\nhand, targets spatial reasoning by leveraging compressed symbolic representations of spatial relations to\\nreduce token usage.\\n2.2\\nReasoning Models and Underlying Techniques\\nRecent reasoning models have moved beyond early prompting-based CoT techniques by internalizing step-\\nby-step reasoning through SFT and RL. Building structured reasoning paradigms mentioned in Section 2.1,\\nthese models are trained to generate reasoning traces aligned with human-like logic. RL plays a crucial\\n3\\nPublished in Transactions on Machine Learning Research (09/2025)\\nWhat is the answer of 1 plus 2?\\nOverthinking\\nOkay, so I need to figure out what 1 plus 2 is. ...\\nLet me count them out. ... 3.\\nAlternatively, ... 3.\\nWait ...\\n...\\nSo the answer must be 3. (619 tokens)\\nWhen More is Less\\nSafety Risks\\nHigh efficiency\\nAlleviate resource constraints\\nLower costs\\n...\\nFor CoT length, longer is not always better\\nLengthy CoT\\nAttacker\\nWhy We Need Efficient Reasoning \\nAccuracy\\nops/step\\nFigure 3: Motivation for efficient reasoning. (Left) Models often exhibit overthinking, generating unneces-\\nsarily long reasoning chains even for simple tasks. (Middle) Longer reasoning is not always better and may\\nresult in reduced accuracy when excessively verbose. (Right) Lengthy reasoning increases computational\\ncosts and poses safety risks. In addition, improving efficiency helps alleviate resource constraints and lower\\ncosts.\\nrole by optimizing for reasoning quality using reward signals based on correctness, format alignment, and\\nprocess supervision (Xu et al., 2025b; Ouyang et al., 2022; Zhou et al., 2023). Advanced models like OpenAI\\no1 (OpenAI, 2024) are believed to incorporate tree-search strategies (Coulom, 2006) and process reward\\nmodels to guide the exploration of intermediate steps. Others, such as DeepSeek R1 (Guo et al., 2025),\\nemploy rule-based reward functions to reinforce correct reasoning steps.\\n2.3\\nTest-Time Scaling\\nScaling test-time computation (TTC) is another road for enhancing reasoning performance (Snell et al., 2024;\\nZeng et al., 2025b). Scaling can be approached from two complementary dimensions: horizontal and vertical.\\nThe horizontal perspective involves generating multiple samples and selecting the best answer.\\nBest-of-\\nN (Cobbe et al., 2021; Sun et al., 2024a) selects the top-scoring response, while self-consistency (Wang et al.,\\n2022a) identifies the most consistent answer across reasoning chains. The vertical perspective focuses on\\nincreasing the length of a single reasoning path. For example, Self-Refine (Madaan et al., 2023) iteratively\\nimproves an initial response via self-evaluation, while other works (Chen et al., 2024d; Gou et al., 2024)\\nleverage external feedback to guide the refinement process. Additionally, an empirical study (Wu et al.,\\n2025c) investigates the trade-offs between the efficiency and performance of various TTS strategies (e.g.,\\nBest-of-N, weighted voting) under different model sizes and computation budgets, providing practical insights\\nfor further research and deployment.\\n2.4\\nModel Compression\\nModel compression strategies are widely used to reduce the size and computational overhead of models (Han\\net al., 2016). Common approaches include quantization (Gray & Neuhoff, 1998; Frantar et al., 2023a; Lin\\net al., 2024; Xiao et al., 2023), which reduces model size by lowering the precision of model parameters.\\nPruning (LeCun et al., 1989; Ma et al., 2023; Fang et al., 2023; Wang et al., 2021) removes less significant\\nor redundant model parameters to achieve sparsity, reducing model size and inference latency. Unlike the\\nabove techniques, knowledge distillation (Hinton et al., 2015; Wang et al., 2022b; Liu et al., 2019) achieves\\ncompression not by directly modifying the original model, but by transferring knowledge from a larger, well-\\ntrained teacher model to a smaller student model, allowing the student to replicate the teacher’s behavior\\nwhile maintaining comparable performance (see details about model compression in Appendix A.1).\\n2.5\\nWhy We Need Efficient Reasoning\\nEfficiency is a valuable research direction across many fields, and in the context of reasoning, we highlight\\nkey motivations for pursuing efficient reasoning (see Figure 3). Reasoning models often generate excessively\\n4\\nPublished in Transactions on Machine Learning Research (09/2025)\\nTable 1: Performance of efficient reasoning methods on the AIME 24 dataset. † denotes the result of the\\noriginal model, averaged over 5 independent runs.\\nCategory\\nType\\nMethods\\nAcc. / #Tokens\\nBase Model\\nOriginal Model\\n-\\nBaseline†\\n70.67% / 10024\\nDeepSeek-R1-32B\\nShorter\\nRL\\nDAST\\n53.30% / 6337\\nDeepSeek-R1-Distill-Qwen-7B\\nShorter\\nSFT\\nCoT-Valve\\n43.30% / 4630\\nQwQ-32B-Preview\\nShorter\\nSFT\\nTOPS\\n46.00% / 6427\\nQwen2.5-32B\\nSmaller\\nKD\\nMix\\n10.00% / -\\nQwen2.5-3B\\nSmaller\\nKD\\nDLCoT\\n53.30% / 18825\\nQwen2.5-14B\\nSmaller\\nRL\\nOpen-RS\\n46.70% / -\\nDeepSeek-R1-Distill-Qwen-1.5B\\nSmaller\\nRL\\nDeepSacleR\\n43.10% / -\\nDeepSeek-R1-Distill-Qwen-1.5B\\nFaster\\nEfficient self-consistency\\nRPC\\n9.50% / -\\nInternLM-2-MATH-Plus 7B\\nFaster\\nEfficient sampling\\nϕ-Decoding\\n16.67% / -\\nLLaMA3.1-8B-I\\nlong reasoning chains to solve reasoning tasks, even for simple samples, and typically rely on larger model\\nsizes to achieve stronger reasoning performance. For example, answering “What is the answer of 1 plus\\n2?” requires 619 tokens from DeepSeek R1-685B (see Appendix A.2 for details). To further illustrate the\\noverhead, we evaluated four versions of DeepSeek R1 on the AIME 24 dataset and observed consistently huge\\ntoken counts: 15513 for 1.5B, 12377 for 7B, 10854 for 14B, and 10024 for 32B. Additionally, some strategies,\\nsuch as Best-of-N and self-consistency, further scale the decoding process to enhance reasoning performance.\\nThese lead to substantial computational and memory demands. Moreover, overly long reasoning paths can\\naccumulate errors and negatively impact final accuracy (Wu et al., 2025d; Yang et al., 2025c).\\nOn the other hand, efficient reasoning is also essential in real-world applications such as embodied AI (Duan\\net al., 2022), agent systems (Wang et al., 2024a), and real-time platforms (e.g., autonomous driving (Cui\\net al., 2024)). In these scenarios, efficiency enables agents to process sensory inputs in real time, make swift\\nand accurate decisions, and interact seamlessly with dynamic environments.\\nAdditionally, unnecessarily\\nlengthy reasoning may increase safety risks (Kuo et al., 2025; Li et al., 2025d), posing unpredictable threats.\\nThese challenges collectively highlight the limitations of current reasoning models, underscoring the necessity\\nof improving reasoning efficiency.\\n3\\nEfficient Reasoning\\nIn the following, we introduce efficient reasoning methods based on three key categories: shortening long\\nchains of thought, as discussed in Section 3.1; developing small language models with strong reasoning capa-\\nbilities, details of which can be found in Section 3.2; and improving decoding efficiency, which is elaborated in\\nSection 3.3. We present the performance of various efficient reasoning methods on the challenging AIME 24\\ndataset in Table 1 and further provide a latency-based summary of representative methods across categories\\non the GSM8K dataset in Table 5.\\n3.1\\nMake Long CoT Short\\nRecent works have explored various approaches to improve reasoning efficiency by shortening CoT length\\nwithout compromising reasoning performance. Among them, RL with length penalty is widely used for\\nencouraging concise and effective reasoning paths (see Section 3.1.1). Another line of work explores SFT\\nwith variable-length CoT data to improve reasoning efficiency, as discussed in Section 3.1.2. In addition,\\nprompt-driven techniques improve reasoning efficiency by utilizing prompts, with further details available\\nin Section 3.1.3. Finally, we explore latent reasoning, which performs the reasoning process in latent space\\nand drastically reduces CoT length, with details provided in Section 3.1.4. Additionally, Table 2 provides\\nan overview of these methods, showing that most RL-based methods utilize Full FT, while many SFT-\\nbased methods adopt Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA (Hu et al., 2022) to\\nreduce cost. This trend suggests that RL-based methods require more extensive parameter updates, making\\nlightweight adaptation less effective; for latent reasoning, Full FT remains dominant, and these methods\\n5\\nPublished in Transactions on Machine Learning Research (09/2025)\\nTable 2: Overview of efficient reasoning methods in Section 3.1. The speedup ratio is computed by comparing\\neither the latency (L.) or the token count (T.). Avg1 represents the average of Llama-3.2-3B, Gemma2-2B,\\nQwen2.5-3B, Qwen2.5-Math-1.5B, and DeepSeekMath-7B; Avg2 represents the average of GPT-4o, GPT-\\n4o-mini, Yi-lightning, o3-mini, and LLaMA3.1-8B-I.\\nType\\nMethods\\nTraining Scheme\\nAcc. / #Tokens\\nBase Model\\nSpeedup\\nRL\\nO1-Pruner\\nPPO (Freeze FT)\\nGSM8K: 96.50% / 543\\nQwQ-32B\\n1.5 - 2.0 × (L.)\\nRL\\nDAST\\nSimPO (Full FT)\\nMATH-500: 92.60% / 2802 DeepSeek-R1-Distill-Qwen-7B\\n1.6 - 2.2 × (T.)\\nRL\\nAGPO\\nGRPO (Full FT)\\nMATH-500: 77.20% / 463 Qwen2.5-Math-7B\\n1.3 - 1.5 × (T.)\\nRL\\nTHINKPRUNE\\nGRPO (Full FT)\\nMATH-500: 83.90% / 2209 DeepSeek-R1-Distill-Qwen-1.5B\\n1.7 - 2.0 × (T.)\\nRL\\nThink When You Need GRPO (Full FT)\\n-\\n-\\n1.3 × (T.)\\nSFT\\nTokenSkip\\nSFT (LoRA)\\nGSM8K: 78.20% / 113\\nLLaMA3.1-8B-I\\n1.7 - 1.8 × (L.)\\nSFT\\nC3oT\\nSFT (Full FT)\\nGSM8K: 47.10% / -\\nLLaMA2-Chat-13B\\n2.0 × (T.)\\nSFT\\nSelf-Training\\nSFT (Full FT)\\nGSM8K: 78.07% / 176\\nAvg1\\n1.3 - 1.5 × (T.)\\nSFT\\nTALE\\nSFT / DPO (LoRA)\\nGSM8K: 78.57% / 140\\nAvg2\\n1.7 × (T.)\\nSFT\\nCoT-Valve\\nProgressive SFT (LoRA)\\nGSM8K: 95.40% / 289\\nQwQ-32B\\n2.6 × (T.)\\nPrompting\\nConcise CoT\\nTraining-free\\n-\\n-\\n1.9 - 2.0 × (T.)\\nPrompting\\nBreak the Chain\\nTraining-free\\nGSM8K: 74.22% / -\\nChatGPT\\n-\\nPrompting\\nTALE-EP\\nTraining-free\\nGSM8K: 84.46% / 77\\nGPT-4o-mini\\n4.1 × (T.)\\nPrompting\\nCoD\\nTraining-free\\nGSM8K: 91.10% / 44\\nGPT-4o\\n4.7 × (T.)\\nRouting\\nRouteLLM\\nLLaMA3-8B Router\\nGSM8K: 74.82% / -\\nGPT-4\\n1.5 × (T.)\\nRouting\\nSketch-of-Thought\\nDistillBERT Router\\n-\\n-\\n3.6 × (T.)\\nRouting\\nSelf-REF\\nSFT (LoRA)\\nGSM8K: 81.60% / -\\nLLaMA3-8B-I\\n1.2 - 2.0 × (L.)\\nLatent reasoning Implicit-KD\\nSFT (Full FT)\\nGSM8K: 20.00% / -\\nGPT-2 small\\n8.2 × (L.)\\nLatent reasoning SI\\nProgressive SFT (Full FT) GSM8K: 30.00% / -\\nGPT-2 small\\n4.0 - 11.0 × (L.)\\nLatent reasoning CCoT\\nSFT (LoRA)\\nGSM8K: 17.90% / -\\nCCOT & DECODE\\n10.4 - 24.5 × (L.)\\nLatent reasoning SoftCoT\\nSFT (Freeze FT)\\nGSM8K: 85.81% / -\\nQwen2.5-7B-I\\n4.0 - 5.0 × (L.)\\nLatent reasoning CODI\\nSelf-distillation (LoRA)\\nGSM8K: 43.70% / -\\nGPT-2 small\\n2.5 - 2.7 × (L.)\\nLatent reasoning LightThinker\\nSFT (Full FT)\\nGSM8K: 90.14% / -\\nQwen2.5-7B\\nup to 1.4 × (L.)\\nLatent reasoning Coconut\\nProgressive SFT (Full FT) GSM8K: 34.10% / 8\\nGPT-2\\n3.0 × (T.)\\nLatent reasoning Token Assorted\\nSFT (Full FT)\\nGSM8K: 84.10% / 194\\nLLaMA3.1-8B\\n1.2 × (T.)\\noften yield higher speedups, indicating that implicit representations enable more effective compression and\\noffer a higher upper bound compared to explicit reasoning chains.\\n3.1.1\\nReinforcement Learning Helps Efficiency Improvement\\nIncorporating explicit chain length penalty into RL is a natural strategy for shortening reasoning\\nchains (Team et al., 2025; Li et al., 2025a; Arora & Zanette, 2025). L1 (Aggarwal & Welleck, 2025) takes\\nthis further by introducing designated length-constraint instructions into the training data. O1-Pruner (Luo\\net al., 2025a) develops a specialized reward design by utilizing length and accuracy from a reference model\\nas baselines, explicitly rewarding shorter reasoning paths and higher accuracy to ensure efficiency without\\nsacrificing performance. DAST (Shen et al., 2025b) aims to achieve a balanced CoT (i.e., dynamically ad-\\njusting computational resources by allocating more reasoning steps to more challenging questions and fewer\\nto simpler ones). Specifically, it proposes a Token Length Budget (TLB), defined as a weighted sum of\\nthe mean token count in accurate answers and a predefined upper bound on generation length to quantify\\nproblem difficulty, penalizing excessively verbose reasoning for simple questions while encouraging com-\\nprehensive reasoning for complex ones. THINKPRUNE (Hou et al., 2025) designs a length-aware reward\\nfunction that only provides a reward if the correct answer is generated within a specified token budget.\\nThe model is trained using the Group Relative Policy Optimization (GRPO) algorithm with progressively\\ntightened length constraints. Additionally, Think When You Need (Yang et al., 2025b) utilizes pairwise\\ncomparisons to generate rewards based on the relative length and accuracy of reasoning, guiding models to\\nproduce concise yet accurate solutions.\\n3.1.2\\nSupervised Fine-Tuning with Variable-Length CoT Data Helps Efficiency Improvement\\nFollowing a clear fine-tuning pipeline, we organize the discussion of this line of research into two stages: (1)\\nhow variable-length CoT data is constructed and (2) which SFT approach (i.e., standard or progressive) is\\nadopted. For each work, we explicitly address these two questions to facilitate comparison and analysis.\\n6\\nPublished in Transactions on Machine Learning Research (09/2025)\\nHow variable-length CoT data is constructed?\\nTo construct variable-length CoT data, long reasoning\\nchains are commonly generated by prompting LLMs with inputs, whereas the key challenge lies in obtaining\\nthe corresponding shorter reasoning chains.\\nTo address this, existing approaches generally fall into two\\ncategories. The first approach involves compressing existing long reasoning paths into shorter ones. For\\ninstance, TokenSkip (Xia et al., 2025) identifies and skips less important tokens based on their semantic\\ncontribution to the final answer. Distill2-to-1 (Yu et al., 2024) discards reasoning steps entirely, retaining only\\nhigh-quality (input, answer) pairs through consistency-based filtering. C3oT (Kang et al., 2024) leverages\\nGPT-4 as a compressor to shorten chain length by preserving essential reasoning details.\\nAdditionally,\\nSPIRIT (Cui et al., 2025) uses perplexity to evaluate step importance, thus selectively compressing reasoning\\npaths.\\nThe alternative approach directly generates short reasoning paths. Self-training (Munkhbat et al., 2025)\\nemploys multiple sampling combined with few-shot prompting, selecting the shortest correct reasoning paths.\\nTALE (Han et al., 2024) observes that LLMs naturally follow token budget constraints specified in prompts\\nand introduces a binary search-based algorithm to identify the optimal token budget for generating concise\\nreasoning paths. TOPS (Yang et al., 2025c) begins with a small set of o1-like responses (i.e., either generated\\nby existing models or manually constructed) as seed data. Each response corresponds to a different level\\nof reasoning effort. Using this data, it trains a tag model that learns to produce variable-length reasoning\\npaths conditioned on effort-specific prompts, enabling the construction of diverse CoT data with controllable\\nlengths. Inspired by model merging (Yang et al., 2024b), CoT-Valve (Ma et al., 2025) achieves chain length\\ncontrol by adjusting a specific direction of the parameter space, merging parameters from a base LLM with\\nthose of a reasoning-enhanced model of identical architecture1. Additionally, LLM-Skip (Liu et al., 2024b)\\nmanually shortens reasoning paths for complex datasets at the initial training stage, explicitly labeling\\nprompts with “Solve it in n steps.”. In the subsequent progressive SFT process, shorter reasoning paths\\ngenerated by the model are continuously integrated into the training set.\\nWhich SFT approach is adopted?\\nMost works adopt a standard SFT approach (Xia et al., 2025; Yu\\net al., 2024; Kang et al., 2024; Cui et al., 2025; Munkhbat et al., 2025; Han et al., 2024; Ma et al., 2025; Yang\\net al., 2025c), typically leveraging either LoRA (Xia et al., 2025; Ma et al., 2025) or full fine-tuning (Kang\\net al., 2024). Notably, C3oT (Kang et al., 2024) designs a conditioned training strategy, enabling the model to\\nlearn both long and short reasoning styles during training and generate concise reasoning paths at inference\\nby simply appending a short condition in the prompt. TALE (Han et al., 2024) further explores DPO as an\\nalternative fine-tuning objective, allowing direct control over the model’s output preference.\\nAnother line of work adopts progressive fine-tuning strategies (Liu et al., 2024b; Ma et al., 2025). LLM-\\nSkip (Liu et al., 2024b) iteratively encourages the model to generate shorter reasoning paths and then merges\\nthe generated shorter paths into the training set for subsequent fine-tuning rounds, gradually reducing chain\\nlength. CoT-Valve (Ma et al., 2025) supports both standard SFT and two progressive strategies: CoT-\\nValve++ and CoT-Valve+P. CoT-Valve++ introduces a normalized path-length factor β, which is smaller\\nfor longer paths. During training, the model parameters are dynamically adjusted along a direction scaled by\\nβ, allowing the model to adapt to reasoning paths of varying lengths and learn finer-grained length control.\\nCoT-Valve+P, on the other hand, progressively trains the model on samples sorted from long to short chains,\\nguiding it to shorten the chain length over successive fine-tuning stages.\\n3.1.3\\nPrompt-Driven Efficiency Enhancement in Reasoning\\nWe categorize prompt-driven works into two directions: (1) prompt-guided reasoning, which leverages well-\\ndesigned prompts to guide reasoning models toward more effective reasoning paths and (2) prompt-based\\nrouting, which utilizes prompt-level attributes (e.g., complexity) to adaptively select appropriate computa-\\ntional paths (e.g., route easy questions to lightweight models and hard ones to powerful large models).\\n1Model merging is an effective strategy for efficient reasoning. For example, Kimi k1.5 (Team et al., 2025) improves token\\nefficiency by merging a long-cot model and a short-cot model, while Wu et al. (2025a) combines System 1 and System 2 models\\nto shorten response length.\\n7\\nPublished in Transactions on Machine Learning Research (09/2025)\\nPrompt-guided Efficient Reasoning.\\nConcise CoT (Renze & Guven, 2024) shows that simply adding\\n“Be concise” to the prompt can shorten reasoning chains. Break the Chain (Ding et al., 2024) leverages\\ncarefully crafted instructions (e.g., “rapidly evaluate and use the most effective reasoning shortcut”) to trigger\\nthe model’s ability to exploit shortcuts and skip unnecessary steps. TALE-EP (Han et al., 2024) employs\\nan LLM-based estimator to predict the minimal token budget required for each question, which is then\\nincorporated into the prompt to guide efficient reasoning. CoD (Xu et al., 2025c) develops the instruction\\n“Think step by step, but only keep a minimum draft for each thinking step, with 5 words at most.”, which\\nsignificantly reduces token usage under few-shot settings without compromising accuracy.\\nHowever, its\\nperformance degrades in zero-shot settings and on small language models.\\nMARP (Chen et al., 2024a)\\nboosts per-step information density and reduces step count under a fixed reasoning boundary, achieving\\nhigh efficiency gains through prompt design, and can be further combined with PoT for better computation-\\nreasoning separation. Token-Complexity (Lee et al., 2025) presents token complexity to measure the minimal\\ntokens needed for correct reasoning and derives the theoretical compression limit of CoT chains. Through\\nprompt variations (e.g., “use 10 words or less” or “remove all punctuation”), they explore the trade-off\\nbetween performance and efficiency and show that current methods still fall far from the optimal bound,\\nleaving room for improvement. Additionally, these methods can effectively construct variable-length CoT\\ndata, thereby supporting the approaches introduced in Section 3.1.2.\\nPrompt Attribute-Aware Efficient Reasoning.\\nClaude 3.7 Sonnet (Anthropic., 2025) offers two re-\\nsponse modes (e.g., quick answers or step-by-step thinking), allocating more compute to complex reasoning\\ntasks. Although the implementation details remain undisclosed, it is the first hybrid reasoning model and a\\nfoundation for subsequent methods.\\nRouting strategies primarily fall into two categories: classifier-based and uncertainty-based. Classifier-based\\napproaches train a separate router to categorize incoming questions and route them to the most suitable\\nmodel. RouteLLM (Ong et al., 2024) trains a router using preference data to dispatch easy questions to\\nlightweight and harder ones to stronger models. Sketch-of-Thought (Aytes et al., 2025) routes each input\\nto the most appropriate reasoning pattern by referencing cognitive science (Goel, 1995), introducing three\\nheuristic modes: Conceptual Chaining, which links ideas using minimal language; Chunked Symbolism, which\\norganizes reasoning into symbolic blocks; and Expert Lexicons, which leverage domain-specific shorthand.\\nUncertainty-based methods rely on confidence to guide routing. Self-REF (Chuang et al., 2024) adds two\\nspecial tokens (i.e., <CN> for confident and <UN> for unconfident) to indicate confidence, training the\\nmodel on annotated responses to self-assess its confidence level. If uncertain, the model defers to a more\\npotent model or abstains. Confident or Seek Stronger (Chuang et al., 2025) further analyzes uncertainty-\\nbased routing, observing that uncertainty distributions are relatively stable across tasks but vary significantly\\nacross models and uncertainty quantification (UQ) methods. It further designs a calibrated data construction\\nstrategy that improves the reliability of routing decisions for small language models.\\n3.1.4\\nReasoning in Latent Space\\nUnlike explicit CoT reasoning, latent reasoning (Deng et al., 2023; Tan et al., 2025) performs the reasoning\\nprocess in latent space, skipping the generation of explicit intermediate steps.\\nLatent reasoning brings\\ntwo key benefits: it allows for more human-like thinking by modeling complex ideas beyond language, and\\nimproves efficiency by reducing the need for explicit reasoning chains. This section first examines how models\\ntransition from explicit to implicit reasoning. Then, we explore how reasoning is represented in latent space.\\nFrom Explicit CoT to Implicit CoT.\\nAs the seminal work introducing implicit CoT, Implicit-KD (Deng\\net al., 2023) proposed a distillation-based framework where a student model learns to reason implicitly by\\nmimicking the hidden states across different layers of an explicit CoT teacher. To eliminate the reliance\\non the teacher model during inference, they further trained a simulator that directly maps input to teacher\\nhidden states.\\nSI (Deng et al., 2024) progressively removes intermediate reasoning steps through SFT,\\nenabling the model to internalize reasoning without explicit chains. Similarly, Distill2-to-1 (Yu et al., 2024)\\nshowed that SFT on (input, answer) pairs alone can yield strong implicit reasoning capabilities. CODI (Shen\\net al., 2025c) introduces a novel self-distillation framework where a shared model acts both as teacher and\\n8\\nPublished in Transactions on Machine Learning Research (09/2025)\\nstudent—explicit CoT is learned via language modeling, while implicit CoT is learned by aligning the hidden\\nactivation of the token intermediately preceding the answer. LightThinker (Zhang et al., 2025a) proposes a\\ndynamic compression strategy for CoT. It segments the reasoning chain and compresses each step into special\\ntokens, with a focus on the KV cache compression. These latent representations are used for subsequent\\nreasoning, with attention masks designed to ensure the model can only access compressed content rather\\nthan whole previous steps.\\nAnother line of work explores using an auxiliary model to generate latent reasoning tokens directly from the\\ninput. CCoT (Cheng & Van Durme, 2024) trains a lightweight CCOT module (a LoRA (Hu et al., 2022))\\nto produce compressed latent reasoning tokens directly from input, which are then fed into a decoding\\nmodule to generate concise answers, while HCoT (Liu et al., 2024c) adopts a similar pipeline but places\\ngreater emphasis on semantic alignment during compression. SoftCoT (Xu et al., 2025d) adopts a similar\\nstrategy by training a lightweight assistant model to produce implicit representations conditioned on the\\ninput. Furthermore, Reasoning with Latent Thoughts (Saunshi et al., 2025) demonstrated that looping a\\ntransformer multiple times could emulate a deeper model and naturally induce latent thoughts, effectively\\ncapturing iterative reasoning without tokenized steps. RELAY (Yu et al., 2025a) follows this idea by aligning\\neach iteration of a looped transformer (Giannou et al., 2023) with explicit CoT steps. The trained looped\\nmodel is then leveraged to produce high-quality CoT chains to train stronger autoregressive models on long\\nreasoning tasks.\\nLatent Space Representations for Reasoning.\\nA common choice for latent space representation is to\\nuse continuous tokens (Zhang et al., 2025a; Shen et al., 2025c; Cheng & Van Durme, 2024; Xu et al., 2025d;\\nHao et al., 2024; Liu et al., 2024c), which naturally align with the internal computation of neural networks.\\nCoconut (Hao et al., 2024) models reasoning in the hidden space by feeding the final-layer hidden states\\nback into the model without decoding explicit CoT tokens, enabling more continuous and efficient reasoning.\\nThis approach unlocks advantages that explicit CoT cannot offer, such as backtracking and parallel decoding.\\nInspired by Coconut, Heima (Shen et al., 2025a) introduces thinking tokens into multimodal large language\\nmodels (MLLMs) to replace explicit reasoning steps, enabling reasoning in the latent space.\\nAnother alternative approach is to employ discrete tokens as explicit representations of intermediate rea-\\nsoning stages. Planning-Token (Wang et al., 2024c) employs a set of planning tokens inserted before each\\nreasoning step to guide the model to generate a latent plan before producing the detailed explanation. These\\ntokens are obtained by clustering the hidden states of reasoning steps, yielding semantically meaningful and\\ndistinct discrete representations. Filler-Token (Pfau et al., 2024) proposes inserting meaningless filler tokens\\n(e.g., repeated dots) into the reasoning path, allowing the model to perform additional hidden computation,\\nthereby enhancing performance on reasoning tasks. Token Assorted (Su et al., 2025) improves reasoning\\nefficiency by mixing text tokens with latent tokens obtained through VQ-VAE (Van Den Oord et al., 2017),\\nreducing sequence length while preserving key information. Disentangling-Memory-and-Reasoning (Jin et al.,\\n2024a) introduces explicit discrete markers such as 〈memory〉and 〈reason〉, which enable the model to dis-\\nentangle reasoning into separate phases (i.e., retrieving relevant knowledge and performing logical inference)\\nwithin the latent space. This separation facilitates more structured and interpretable reasoning behaviors.\\n3.2\\nBuild Small Language Model with Strong Reasoning Ability\\nCompared to compressing reasoning chains, an alternative approach to improving reasoning efficiency is to\\nempower small language models (SLMs) with strong reasoning capabilities. Due to their lower memory\\nand computational requirements, SLMs are inherently more efficient and easier to deploy in real-world\\napplications. Model compression (Han et al., 2016; Frantar et al., 2023b; Li et al., 2023b) naturally aligns\\nwith this goal, as it enables small or compressed models to retain or gain reasoning abilities. A natural\\nstarting point is to transfer reasoning capabilities from larger models via distillation (see Section 3.2.1).\\nWe further explore other model compression techniques, including pruning and quantization, which aim to\\ncompress models without severely compromising reasoning performance in Section 3.2.2. Beyond traditional\\nmodel compression techniques, RL offers another promising direction, enhancing reasoning capabilities under\\nlimited resources through carefully designed training strategies, as discussed in Section 3.2.3. Additionally,\\na summary of these methods is presented in Table 3, indicating that most distillation approaches still rely\\n9\\nPublished in Transactions on Machine Learning Research (09/2025)\\nTable 3: Overview of efficient reasoning methods in Section 3.2. Blended1 represents the combination of\\ns1 and DeepSacleR datasets; Blended2 represents the combination of Omni-MATH, AIME, AMC, and Still\\ndatasets.\\nType Methods\\nTraining Scheme\\nTraining Data\\nAcc.\\nBase Model\\nKD\\nCoT-KD\\nDistillation (Full FT)\\nCoT data\\nGSM8K: 21.99% (↑13.88%) T5 XXL\\nKD\\nMD\\nMixed distillation (Freeze FT)\\nCoT and PoT data\\nGSM8K: 41.50% (↑28.20%) LLaMA2-7B\\nKD\\nMix\\nMixed distillation (Full FT & LoRA) Long and short CoT data\\nGSM8K: 79.20% (↑1.70%) LLaMA3.2-3B\\nKD\\nNAT\\nMixed distillation (LoRA)\\nPositive and negative data\\nGSM8K: 41.24% (↑23.73%) LLaMA-7B\\nKD\\nCD\\nCounterfactual distillation (Full FT)\\nOriginal and counterfactual data -\\n-\\nKD\\nFDD\\nFeedback-driven distillation (Full FT) Progressively add generated data GSM8K: 49.43% (↑42.53%) FlanT5-Large\\nKD\\nDLCoT\\nDistillation (Full FT)\\nHigh-quality data\\nGSM8K: 93.60% (↑9.10%) LLaMA3.1-8B\\nKD\\nSKIntern\\nDistillation (LoRA)\\nProgressively simplify data\\nGSM8K: 33.90% (↑30.80%) LLaMA2-7B\\nRL\\nOpen-RS\\nGRPO (Full FT)\\nBlended1\\nAIME: 46.70% (↑17.80%)\\nDeepSeek-R1-Distill-Qwen-1.5B\\nRL\\nDeepSacleR GRPO (Full FT)\\nBlended2\\nAIME: 43.10% (↑14.20%)\\nDeepSeek-R1-Distill-Qwen-1.5B\\non Full FT, with a few adopting PEFT techniques. Notably, methods that progressively incorporate refined\\nor synthesized data (e.g., FDD and SKIntern) tend to achieve greater performance improvements.\\nApart from model compression and RL, some studies explore the reasoning ability of small language models\\nfrom alternative perspectives. For example, Liu et al. (2025d) shows that small language models can match\\nor even surpass the reasoning performance of much larger LLMs with carefully designed TTS strategies.\\nHowever, the effectiveness of TTS strategies varies with model architecture, reward design, and task com-\\nplexity. While small language models show potential in reasoning, their limitations in instruction following\\nand self-reflection highlight the need for further adaptation to align with human intent.\\n3.2.1\\nDistillation Transfers Reasoning Ability to Small Language Model\\nCoT-KD (Magister et al., 2022) first demonstrated that distillation can transfer reasoning ability from\\nLLMs to small language models. However, due to limited capacity, small language models struggle to learn\\ncomplex reasoning (Li et al., 2025e), motivating the development of more advanced strategies. Based on the\\noptimization target, existing methods can be grouped into two directions: (1) data-focused, which improves\\nthe quality or composition of training data, and (2) model-focused, which concentrates on the distilled model\\nitself or its generation strategy.\\nData-focused.\\nMD (Li et al., 2023a) adopts mix distillation by combining data generated with different\\nprompting strategies (CoT and PoT) as training data, and Mix (Li et al., 2025e) applies a similar strategy\\nusing a mix of long and short CoT samples. CD (Feng et al., 2024c) enhances training diversity by mixing\\noriginal data with counterfactual samples derived from it, while NAT (Li et al., 2024a) leverages negative\\ndata. DLCoT (Luo et al., 2025c) improves training data quality by segmenting and simplifying long reasoning\\npaths. SCORE (Zhang et al., 2024) enables self-correction by allowing the model to generate, identify, and\\nrefine its reasoning, using the corrected outputs for further distillation. Distill2-to-1 (Yu et al., 2024) only\\nretrains (input, answer) pairs as training data. The above methods rely on standard SFT, but some adopt\\nprogressive SFT. FDD (Zhu et al., 2024b) progressively adjusts data difficulty based on the small language\\nmodel’s performance on LLM-generated data, while SKIntern (Liao et al., 2025b) proposes a progressive\\nprocess that removes symbolic knowledge and examples step by step, encouraging the model to internalize\\nreasoning ability.\\nModel-focused.\\nPRR (Zhao et al., 2024) distills two separate models: a probing model for retrieving\\nrelevant knowledge and a reasoning model for generating answers based on the question and retrieved con-\\ntent. Thinking slow, fast (Paliotta et al., 2025) explores distilling reasoning ability from transformer-based\\nmodels into Mamba or Mamba-Transformer architectures to reduce inference cost. Similarly, M1 (Wang\\net al., 2025b) builds on Mamba (Gu & Dao, 2024) to develop a hybrid linear RNN reasoning model that\\nalleviates latency and memory overhead from long reasoning chains, further enhanced through RL after\\ndistillation. Additionally, works such as NSA (Yuan et al., 2025) and MoBA (Lu et al., 2025), which fo-\\ncus on lightweight architectures for general efficiency, can also be extended to improve reasoning efficiency.\\nAdditionally, ATM (Chen et al., 2024b) designs an adaptive mechanism that enables the student model to\\n10\\nPublished in Transactions on Machine Learning Research (09/2025)\\ndynamically choose between pre-thinking (i.e., thinking before answering) and post-thinking (i.e., answering\\nbefore thinking) based on question complexity.\\n3.2.2\\nPruning or Quantization Retain Reasoning Ability\\nRecent work (Srivastava et al., 2025) systematically explores the impact of compression techniques like\\npruning and quantization on the reasoning capabilities of small language models, which shows that while\\nquantization methods (Frantar et al., 2023b) have minimal impact on reasoning performance, pruning ap-\\nproaches (Li et al., 2023b) significantly degrade reasoning abilities. Similarly, When Reasoning Meets Com-\\npression (Zhang et al., 2025b) presents a comprehensive benchmark of compressed LRMs across various\\nreasoning tasks. It also finds that quantized models retain strong reasoning performance and sometimes\\neven surpass the original model, while aggressive pruning causes performance collapse at moderate spar-\\nsity. Furthermore, Quantization Hurts Reasoning? (Liu et al., 2025c) systematically evaluates the impact\\nof quantization on reasoning models. It finds that high-bit (e.g., 8-bit) quantization is nearly lossless, while\\nlow-bit settings (e.g., 4-bit) significantly degrade performance, especially on complex tasks. Interestingly, the\\noutput length of CoT reasoning remains largely unchanged, except under aggressive quantization or when\\nusing small models. Notably, the results show that on certain large models, quantization can reduce GPU\\nmemory usage by over 75% while retaining nearly 100% of the original performance. Meanwhile, quantized\\nversions of large models are often more effective than standalone small models, offering advantages in both\\nmemory efficiency and performance.\\n3.2.3\\nReinforcement Learning Helps Build Small Language Model\\nSLM-Foresee (Srivastava et al., 2025) conducted a systematic study on the reasoning abilities of diverse small\\nlanguage models, demonstrating that small language models can exhibit strong reasoning potential. Certain\\nmodels, such as the Qwen2.5 series (Yang et al., 2024a), even achieve performance comparable to or surpass-\\ning some LLMs. Open-RS (Dang & Ngo, 2025) enhanced the reasoning capability of small language models\\nusing RL with the GRPO algorithm (Guo et al., 2025) and curated a high-quality mathematical reasoning\\ndataset derived from the s1 dataset (Muennighoff et al., 2025) and DeepScaleR dataset (Luo et al., 2025b).\\nThey further develop a cosine reward to control response length effectively. Their 1.5B model, trained on 7K\\nsamples within 24 hours on 4×A40 GPUs, achieved performance on benchmarks (e.g., AIME 24, MATH-500)\\nthat matches or surpasses models like o1-preview (AI., 2024). SimpleRL-Zoo (Zeng et al., 2025a) systemati-\\ncally evaluated the generality of ZeroRL (i.e., an RL paradigm that enables LMs to learn long-chain reasoning\\nwith only simple rule-based rewards and no additional supervision). The study proposed several key design\\nstrategies for successful ZeroRL training: using simple correctness-based rewards, aligning data difficulty\\nwith model capacity, and employing stable RL algorithms like GRPO. Remarkably, verification behavior\\nwas observed for the first time in small language models outside the Qwen2.5 series2, further validating\\nthe reasoning potential of small language models. Additionally, DeepScaleR3 (Luo et al., 2025b) leverages\\niterative scaling of GRPO to extend thinking length (i.e., 8K →16K →24K), significantly improving per-\\nformance on math reasoning benchmarks. The 1.5B model, DeepScaleR-1.5B-Preview, surpasses o1-Preview\\nand achieves 43.1% Pass@1 on AIME.\\n3.3\\nLet Decoding More Efficient\\nIn the previous sections, we discussed two main directions for improving reasoning efficiency. However, this\\nsection covers strategies to accelerate reasoning during the decoding stage. It begins with techniques to\\nreduce computational overhead during TTS (see Section 3.3.1), followed by an overview of other methods for\\nmaking reasoning faster, with details provided in Section 3.3.2. These methods are summarized in Table 4,\\nshowing that most methods achieve notable efficiency gains and further improve model performance without\\nadditional training.\\n2Most existing works focus exclusively on Qwen2.5 models, whose strong instruction following and self-reflection abilities\\nmay skew results.\\n3DeepScaleR is a reasoning project for small language models, code and models are available at: https://github.com/\\nagentica-project/deepscaler\\n11\\nPublished in Transactions on Machine Learning Research (09/2025)\\nTable 4: Overview of efficient reasoning methods in Section 3.3. The efficiency-up ratio is computed by\\ncomparing either the sampling count (S.), costs (C.), latency (L.), the correct trajectory count (T.), or\\nFLOPs (F.). C1 represents the consistency probability of the majority candidate. C2 means the answer\\nconsistency within the sampling window. C3 is the internal consistency via Chain-of-Embedding. C4 is the\\nprobability of reaching the correct answer.\\nType\\nMethods\\nTraining Scheme Criteria\\nGSM8K ∆Acc. Base Model\\nEfficiency-up Ratio\\nEfficient self-consistency ASC\\ntraining-free\\nC1\\n0.00%\\nGPT-3.5-Turbo\\n1.4 - 4.3 × (S.)\\nEfficient self-consistency ESC\\ntraining-free\\nC2\\n0.00%\\nGPT-4\\n1.3 - 5.0 × (S.)\\nEfficient self-consistency DSC\\ntraining-free\\nC1 + Difficulty ↓0.02%\\nGPT-4\\n2.6 - 5.0 × (C.)\\nEfficient self-consistency Path-Consistency\\ntraining-free\\n-\\n↑3.80%\\nLLaMA3-8B\\n1.2 × (L.)\\nEfficient self-consistency Self-Calibration\\nSFT (Full FT)\\nConfidence\\n↑2.99%\\nLLaMA3.1-8B-I\\n16.7 × (S.)\\nEfficient sampling\\nFast Best-of-N\\ntraining-free\\nReward score\\n-\\n-\\n39.9 × (L.)\\nEfficient sampling\\nST-BoN\\ntraining-free\\nC3\\n-\\n-\\n2.0 × (L.)\\nEfficient sampling\\nFastMCTS\\ntraining-free\\nC4\\n↑1.80%\\nQwen2.5-7B\\n1.1 - 3.0 × (T.)\\nEfficient sampling\\nPredictive-Decoding training-free\\n-\\n↑0.40%\\nLLaMA3-8B\\n-\\nEfficient sampling\\nϕ-Decoding\\ntraining-free\\n-\\n↑6.14%\\nLLaMA3.1-8B-I\\n2.8 × (F.)\\nEfficient sampling\\nSkeleton-of-Thought training-free\\n-\\n-\\n-\\n1.1 - 2.4 × (L.)\\nOther methods\\nAoT\\ntraining-free\\n-\\n↑3.00%\\nGPT-4o-mini-0718\\n-\\n3.3.1\\nEfficiency for Test-Time Scaling Strategy\\nWhile TTS strategies (Snell et al., 2024) have shown great promise in improving reasoning performance\\nwithout modifying model weights, they often cost significant computational overhead. To make TTS more\\nefficient, we categorize this series of works into two directions: (1) efficient sampling methods that optimize\\nthe generation process in sampling-based TTS strategies and (2) efficient self-consistency techniques that\\nreduce the cost of consistency-based reasoning.\\nEfficient Sampling.\\nDuring the sampling process, the quality of generated reasoning chains often varies,\\nand low-quality outputs lead to substantial redundant computation. A key challenge lies in how to allocate\\ncomputation more effectively.\\nA natural solution is to terminate low-quality outputs early.\\nFast Best-\\nof-N (Sun et al., 2024a) proposes speculative rejection, which halts underperforming candidates based on\\nearly-stage partial rewards. ST-BoN (Wang et al., 2025d) adopts early consistency checks to identify and\\nretain high-potential candidates while truncating the rest. Early path evaluation can also be applied to\\nreasoning data synthesis.\\nFastMCTS (Li et al., 2025b) leverages MCTS to build reasoning paths while\\nevaluating quality at each step, allowing for dynamic path adjustment.\\nAnother line of work explores\\npredicting the future trajectory to reduce redundancy and improve overall quality.\\nInspired by Model\\nPredictive Control (Qin & Badgwell, 1997), Ma et al. (2024) proposes Predictive-Decoding, which mitigates\\nthe myopic nature of token-level generation in CoT by simulating several future reasoning steps (i.e., foresight\\ntrajectories) to reweight the token distribution. Similarly, Mendes & Ritter (2025) trains a value model from\\nthe language model’s step-by-step generation dynamics to estimate the utility of intermediate reasoning\\nstates and decide whether to proceed. ϕ-Decoding (Xu et al., 2025a) takes a step further by simulating\\nmultiple future paths at each step, clustering them to form a representative distribution and sampling the\\nnext step from this estimate.\\nBeyond token-level sampling, recent efforts have focused on structured sampling strategies within multi-\\npath reasoning frameworks such as ToT and SoT. DPTS (Ding et al., 2025) proposes a Dynamic Parallel\\nTree Search framework that parallelizes reasoning path generation and dynamically manages cache states,\\nenabling flexible path switching without deep exploration.\\nIt also incorporates early path evaluation to\\nprioritize promising branches. Similarly, FETCH (Wang et al., 2025a) improves efficiency by merging se-\\nmantically similar reasoning states to avoid redundant exploration and applying Temporal Difference (TD)\\nlearning (Sutton, 1988) with λ-return to stabilize verifier scores, reducing unnecessary switching.\\nEfficient Self-Consistency.\\nSelf-consistency also relies on repeated sampling, which leads to substantial\\ncomputational overhead.\\nIts core challenge aligns with efficient sampling—how to allocate computation\\nadaptively. ASC (Aggarwal et al., 2023) estimates answer confidence during sampling and stops early once\\nsufficient confidence is observed, while ESC (Li et al., 2024b) divides the sampling process into sequential\\n12\\nPublished in Transactions on Machine Learning Research (09/2025)\\nwindows and stops sampling as soon as one window yields unanimous answers. DSC (Wang et al., 2024b)\\nfurther incorporates difficulty awareness to better adjust the sample budget per instance.\\nRASC (Wan\\net al., 2024) develops a similar early-stopping mechanism, terminating once sufficient high-quality samples\\nare collected, followed by a score-weighted vote to determine the final answer. RPC (Zhou et al., 2025)\\ncombines self-consistency with perplexity-based estimation to accelerate convergence (i.e., the rate at which\\nconfidence estimation error for the final answer decreases with more samples). It also applies reasoning\\npruning to eliminate low-probability reasoning paths, reducing redundant computation.\\nCISC (Tauben-\\nfeld et al., 2025) augments each sampled response with a model-predicted confidence score and performs\\nconfidence-weighted voting to improve final accuracy under the same sampling budget. Following the same\\nidea, Self-Calibration (Huang et al., 2025) distills consistency signals from self-consistency into the model\\nitself, enabling it to predict confidence scores during inference. This confidence is then used to guide early-\\nstopping policies. Lastly, Path-Consistency (Zhu et al., 2024a) extracts high-confidence reasoning prefixes\\nfrom early samples and reuses them to guide future sampling, improving generation speed and answer quality.\\n3.3.2\\nOther Methods for Making Reasoning Faster\\nOne common approach is to decompose the original problem into sub-problems, reducing redundant token\\ngeneration and skipping uninformative reasoning paths. AoT (Teng et al., 2025) constructs a DAG to model\\nthe dependencies among initially decomposed sub-problems. It then solves the overall task by iteratively\\ndecomposing and merging sub-problems. At each step, the model only processes a simplified version of the\\nproblem, reducing unnecessary token usage, minimizing attention overhead, and avoiding memory issues\\ncaused by long contexts. DISC (Light et al., 2025) dynamically partitions the problem into sub-steps and\\napplies reward-based dynamic sampling and early stopping for each step to control compute costs, achieving\\nefficient inference. AR (Liu et al., 2025b) decomposes the reasoning process into atomic reasoning actions\\norganized into an atomic tree and performs structured reasoning via cognitive routing (e.g., reflection,\\nbacktracking, and termination). This atomic reasoning paradigm has also proven effective in multimodal\\nlarge language models (MLLMs) (Xiang et al., 2025b). SoT (Ning et al., 2023) employs a two-stage decoding\\nstrategy by generating a reasoning skeleton and filling nodes in parallel. Inspired by SoT, SGD (Jin et al.,\\n2024c) further builds a graph over sub-questions to capture logical dependencies and introduces difficulty-\\naware strategies to enable more efficient and higher-quality parallel decoding of reasoning models.\\nIn real-world applications, LLMs are expected to adapt their output length to input complexity, producing\\ndetailed reasoning for complex tasks and concise responses for simpler ones. Several methods have been\\nproposed to achieve this. TTC-Optimal Scaling (Snell et al., 2024) proposes a test-time compute-optimal\\nscaling strategy that first estimates the difficulty of a prompt (i.e., either via oracle or model-predicted\\ndifficulty) and then adaptively selects different TTS strategies. For instance, on easy questions where the\\ninitial response is likely close to correct, self-verification is more efficient than multiple sampling; for complex\\nproblems, tree search with a verifier helps explore diverse reasoning paths. MRT (Qu et al., 2025b) further\\nimproves efficiency by introducing dense rewards based on reasoning progress (i.e., rewarding steps that\\nincrease the likelihood of reaching a correct answer) and training LLMs to progress toward solutions and\\navoid unnecessary computation.\\nRSD (Liao et al., 2025a) enhances reasoning efficiency by combining a\\nsmaller draft model with a larger target model guided by a reward function. The draft model generates\\ncandidate steps, and if the reward is high, the output is accepted; otherwise, the target model refines it.\\nInspired by meta-cognition (Gao et al., 2024), Meta-Reasoner (Sui et al., 2025c) acts as a strategic advisor to\\nguide the reasoning process, evaluate reasoning progress, and provide high-level guidance (e.g., backtracking,\\nrestarting) based on task complexity. Additionally, SpecReason (Pan et al., 2025) leverages the semantic\\ntolerance in reasoning processes by using a lightweight model to speculate intermediate steps while reserving\\nthe large model for verification and correction.\\n3.4\\nA Supplementary: Intersections and Synergies Across Efficient Strategies.\\nEfficient reasoning strategies are not isolated, many methods combine ideas across categories to achieve\\nbetter performance and flexibility. Distillation, beyond transferring reasoning capabilities, also serves as\\nan effective means to realize latent reasoning (Deng et al., 2023; Shen et al., 2025c; Yu et al., 2024). Its\\ncore idea further supports SFT-based methods by enabling the student model to mimic multi-step reasoning\\n13\\nPublished in Transactions on Machine Learning Research (09/2025)\\npatterns (Kang et al., 2024; Munkhbat et al., 2025). Additionally, SFT and RL can be combined for adaptive\\nreasoning. SFT is used to teach the model different answering modes, while RL helps the model learn when\\nto switch among them based on input difficulty (Fang et al., 2025; Wu et al., 2025b).\\n4\\nEvaluation and Benchmark\\n4.1\\nMetrics\\nAssessing reasoning efficiency requires diverse metrics reflecting computational costs and model performance\\n(e.g., accuracy). These metrics provide insights into the trade-offs between computational efficiency and\\nmodel capability, moving beyond traditional evaluation methods that solely focus on performance by in-\\ncorporating additional criteria such as token count, model size, and inference latency.\\nIn the following\\nparagraphs, we present metrics for evaluating reasoning efficiency from both general and reasoning-specific\\nperspectives. For the general perspective, we focus on metrics related to memory, computation, and power.\\nFor the reasoning-specific perspective, we first review classic metrics used to assess reasoning capability and\\nthen discuss metrics tailored specifically for reasoning efficiency.\\n4.1.1\\nGeneral Perspective\\nMemory.\\n• Model Size is a critical factor influencing its storage requirements and computational demands.\\nIt is commonly measured in megabytes (MB) or gigabytes (GB) and is particularly important for\\ndeployment in resource-constrained environments. Several key factors contribute to a model’s size,\\nincluding parameter count, data type, and specific architectural design choices.\\n• Memory Footprint refers to the amount of Random Access Memory (RAM) required to run\\na model during training or inference. This metric is essential for understanding the model’s re-\\nsource demands, particularly in environments with limited memory capacity, such as edge devices\\nor lightweight servers. Memory is measured in units like MB or GB and is primarily determined by\\nthe model size and additional temporary data (e.g., intermediate variables).\\nComputation.\\n• Floating Point Operations (FLOPs) measures the number of floating-point arithmetic opera-\\ntions a model performs during inference or training. This metric reflects a model’s computational\\ncomplexity and is commonly used to assess its efficiency.\\n• Latency (i.e., inference time) measures the time required for an LLM to generate a response\\nafter receiving an input. This metric reflects the model’s responsiveness and is particularly important\\nin real-world applications (e.g., chatbots) where timely outputs are essential. Latency is typically\\nmeasured in seconds (s) and depends on hardware capabilities, model size, and system optimizations.\\nAdditionally, latency can be evaluated in two key ways: end-to-end latency, which measures the total\\ntime from receiving an input to producing the final output, and next-token latency, which assesses\\nthe time required to generate each token in autoregressive models.\\n• Throughput measures an LLM’s efficiency by the number of tokens generated per second, typically\\nexpressed as tokens per second (TPS). It indicates overall processing capability and is crucial for\\nbatch processing or large-scale deployments. For concurrent request scenarios, throughput can be\\nexpressed as queries per second (QPS).\\nPower.\\n• Power Cost refers to the total energy consumed by an LLM throughout its lifecycle, typically\\nmeasured in Watt-hours (Wh) or Joules (J). It reflects the energy usage of key hardware components\\nsuch as GPUs, CPUs, and DRAM.\\n14\\nPublished in Transactions on Machine Learning Research (09/2025)\\n• Carbon Emission measures the environmental impact of LLMs by quantifying the greenhouse\\ngases produced during their life cycle. It is typically expressed in kilograms (kg) or tons of CO2\\nequivalent (CO2eq) and is influenced by factors such as hardware efficiency and model runtime.\\nCarbon emissions can be estimated as follows (see Appendix A.4.1 for the formula). Several tools4\\nare providing real-time emission tracking (e.g., CodeCarbon (Schmidt et al., 2021) and Carbon-\\nTracker (Anthony et al., 2020)) and predicting environmental costs (e.g., MLCO2 Impact (Lacoste\\net al., 2019)).\\n4.1.2\\nReasoning-specific Persective\\nFor reasoning evaluation, several accuracy variants are used. For example, greedy accuracy measures the\\naccuracy when decoding deterministically (i.e., selecting the most likely token at each step). Minimum-\\nmaximum spread (Atil et al., 2024) quantifies stability by computing the accuracy gap across multiple\\nruns. To better evaluate potential performance, the widely used Pass@k, which was initially proposed for\\ngenerated code (Chen et al., 2021), has been adopted for reasoning tasks (Luo et al., 2023; Yu et al., 2023).\\nIt measures the probability of obtaining at least one correct answer among k independent model outputs\\n(see Appendix A.4.2 for the formula).\\nTo capture stability, Pass∧k (Yao et al., 2024) is proposed, which measures the probability that all k\\ngenerations are correct (see Appendix A.4.3 for the formula). Pass∧k forms the basis for G-Pass@kτ (Liu\\net al., 2024a), which further incorporates a tolerance threshold τ, requiring only a minimum proportion of\\ncorrect responses among the k outputs. Furthermore, to jointly assess potential and stability, mG-Pass@kτ\\ninterpolates G-Pass@kτ over the interval [0.5, 1.0], producing a comprehensive metric (see Appendix A.4.4\\nfor formulas).\\nThese metrics provide a complete view of LLM reasoning performance, balancing one-shot potential with\\nconsistency across trials. Additionally, Total Agreement Rate@N (TAR@N) (Atil et al., 2024) evaluates the\\nconsistency of a model by running it N times and measuring how often it produces identical outputs. It has\\ntwo variants: TARa@N, which checks for agreement in the final answers, and TARr@N, a stricter version\\nthat requires an exact string-level match of the full outputs across runs.\\nTo assess reasoning efficiency, token count (i.e., the number of output tokens generated by the model) is com-\\nmonly used as an evaluation metric. Some studies have proposed composite metrics that integrate multiple\\ndimensions of reasoning efficiency. CoT-Valve (Ma et al., 2025) proposes Accuracy per Computation Unit\\n(ACU), calculated as accuracy divided by the product of parameter count and token count, explicitly con-\\nsidering the trade-offs among reasoning path length, model size, and model performance. Chen et al. (2024c)\\nproposes two metrics: the outcome efficiency metric and the process efficiency metric (see Appendix A.4.5\\nfor formulas). The outcome efficiency metric evaluates the proportion of efficient tokens (i.e., the tokens\\nused until the first correct answer is produced) in the model-generated outputs. In contrast, the process\\nefficiency metric assesses the diversity of reasoning paths within generated solutions.\\nAdditionally, Cuadron et al. (2025) introduced the overthinking score, a reliable metric explicitly designed\\nfor quantifying the degree of overthinking in LLMs. The score is obtained using an LLM-based evaluator\\ncombined with structured prompt templates. Chen et al. (2024a) proposed the reasoning boundary (RB) to\\nquantify the upper limit of LLM capability in handling complex reasoning tasks (see Appendix A.4.6 for the\\nformula). Wang et al. (2025e) proposed the underthinking metric to evaluate whether a model prematurely\\nabandons effective reasoning paths in incorrect responses, resulting in a large number of unproductive tokens\\n(see Appendix A.4.7 for the formula).\\nPreference for Metrics: Trade-off between Performance and Efficiency.\\nIn most efficient reasoning\\nstudies, performance and efficiency are typically evaluated separately—performance is measured by accuracy\\nor Pass@k, while efficiency is assessed via token count, latency, or model size. This decoupled evaluation is\\nsimple and effective. However, some recent works have proposed unified metrics that jointly capture both\\naspects. For example, CoT-Valve (Ma et al., 2025) introduces ACU, which combines parameter count, token\\ncount, and accuracy into a single metric. TALE (Han et al., 2024) proposes the optimal token budget, defined\\n4An online calculator: https://mlco2.github.io/impact/\\n15\\nPublished in Transactions on Machine Learning Research (09/2025)\\nas the minimum number of tokens required to maintain correctness, and uses search algorithms to guide the\\nmodel toward more efficient reasoning. Moving forward, there is a growing need for better evaluation metrics\\nthat can balance performance and efficiency more holistically and practically. O1-Pruner (Luo et al., 2025a)\\nproposes a novel metric called the Accuracy Efficiency Score (AES), which considers both the solution length\\nand model accuracy and penalizes accuracy degradation more than it rewards improvement (see more details\\nin Appendix A.4.8).\\n4.2\\nDatasets and Benchmarks\\nDatasets and benchmarks are crucial in evaluating language models’ reasoning capabilities and efficiency.\\nThey provide standardized protocols for assessing how well models can perform reasoning tasks under various\\nresource constraints, such as limited computing or inference budgets. These resources cover a broad spec-\\ntrum of reasoning types—including mathematical, logical, and multi-hop reasoning—enabling comprehensive\\nevaluation across diverse domains and difficulty levels (see more details in Table 6).\\nDatasets.\\nTo evaluate LLM reasoning ability, researchers commonly utilize developing reasoning bench-\\nmarks and datasets. Datasets are commonly categorized based on underlying reasoning types (Parashar\\net al., 2025), such as math reasoning (e.g., GSM8K (Cobbe et al., 2021), PRM800K (Lightman et al., 2023),\\nMATH & MATH-500 (Hendrycks et al., 2021), AIME, and AQuA (Ling et al., 2017)), logical Reasoning\\n(e.g., ProntoQA (Saparov & He, 2023)), common sense reasoning (e.g., StrategyQA (Geva et al., 2021), Hot-\\nPotQA (Yang et al., 2018)), algorithmic reasoning (e.g., Game of 24 (Yao et al., 2023), Bin Packing (Parashar\\net al., 2025)), and planning (e.g., BlocksWorld (Valmeekam et al., 2023), Rubik’s Cube (Ding et al., 2023),\\nTrip Plan, and Calendar Plan (Zheng et al., 2024)).\\nBenchmarks.\\nSys2Bench (Parashar et al., 2025) is a benchmark suite designed for evaluating LLMs,\\ncomprising 11 datasets that cover five categories of reasoning abilities (arithmetic, logical, commonsense,\\nalgorithmic, and planning). In addition to general reasoning benchmarks, several specialized benchmarks\\nhave emerged to evaluate some special situations. Overthinking Bench (Cuadron et al., 2025) proposed a\\nframework to assess the extent of overthinking in LLMs. Analyzing 4,018 trajectories revealed that LLMs\\nprefer extended internal reasoning rather than environmental interactions, and it identified several undesir-\\nable behavioral patterns, such as Analysis Paralysis, Rogue Actions, and Premature Disengagement. Bag\\nof Tricks (Liu et al., 2025a) evaluates explicitly the impact of TTC techniques on the reasoning abilities of\\nLLMs and presents a benchmark covering six test-time optimization strategies evaluated on eight reasoning\\ntasks. DNA Bench (Hashemi et al., 2025) is a benchmark to assess the over-reasoning problem prevalent\\nin current reasoning models. It comprises 150 adversarial prompts covering four key challenges (e.g., in-\\nstruction adherence, hallucination avoidance, redundancy filtering, and unanswerable question recognition).\\nDNA Bench highlights that reasoning models often produce redundant or invalid responses to simple yet\\nmisleading tasks, causing unnecessary computation and reduced accuracy.\\n5\\nDiscussions and Future Directions\\nEfficiency Up Brings Safety Down?\\nWhile long CoT has been shown to enhance reasoning capabilities,\\nH-CoT (Kuo et al., 2025) reveals that LRMs can be exploited via extended CoT paths to bypass safety\\nguardrails (Feng et al., 2024a), leading to harmful outputs (Li et al., 2025d). This suggests a tension between\\nsafety and efficiency: enhancing safety requires longer, more deliberate reasoning for self-correction, which\\nundermines efficiency, while shorter, efficient reasoning paths may skip critical safety checks. Balancing\\nsafety and efficiency remains a crucial challenge for future research in LLM reasoning. Latent reasoning\\noffers a more structured, compact, and controllable process, making it a promising direction for reducing\\nsafety risks. Additionally, representation alignment, which constrains internal representations, may serve as\\na lightweight yet effective strategy for enhancing model safety.\\nEfficient Reasoning for Multimodal Large Language Model.\\nSome efficient reasoning methods\\ncan be naturally extended to the multimodal large language model (MLLM) setting. The decomposition\\nstrategy discussed in Section 3.3.2, which breaks complex tasks into atomic reasoning units, can also benefit\\n16\\nPublished in Transactions on Machine Learning Research (09/2025)\\nmultimodal reasoning (Xiang et al., 2025a; Hu et al., 2025). Similarly, latent reasoning has shown promise in\\nMLLMs (see Heima in Section 3.1.4). LatentLM (Sun et al., 2024b) further explores this direction by unifying\\ndiscrete and continuous modalities through latent language modeling.\\nIt uses a variational autoencoder\\n(VAE) to encode continuous data into latent vectors and then applies next-token diffusion for autoregressive\\ngeneration, enabling scalable and efficient multimodal generation. Additionally, efficient reasoning has been\\nextended to typical vision tasks (Wang et al., 2025c; Koksal & Alatan, 2025; Feng et al., 2025; Li et al.,\\n2025c; Ouyang et al., 2023; Shao et al., 2025), offering valuable insights for future research on integrating\\nstructured reasoning into vision-centric multimodal applications.\\nBreak Memory Limitation.\\nWhile long reasoning paths bring remarkable performance, they also cause\\nsevere memory issues due to long context. PENCIL (Yang et al., 2025a) addresses this by progressively\\nerasing outdated and unimportant reasoning steps during generation. INFTYTHINK (Yan et al., 2025)\\nadopts a segmentation strategy, breaking the reasoning path into shorter fragments and inserting concise\\nintermediate summaries, enabling chunk-wise thinking. OMNIKV (Hao et al., 2025) observes that adjacent\\nlayers share highly similar token importance distributions and thus dynamically select key tokens and reuse\\nthem across subsequent layers. MCoT (Yang et al., 2024c) models multi-step reasoning as a Markov chain,\\nwhere each step depends only on the previous one, avoiding the accumulation of long historical states in the\\nKV cache. These methods show the value of memory-efficient designs; future work should pursue lighter\\narchitectures (Gu & Dao, 2024; Yuan et al., 2025) and adaptive context management for scalable long-range\\nreasoning.\\nTraining Efficiency.\\nTraining long reasoning models remains a computationally intensive task. Recent\\nwork has aimed to improve training efficiency through both curriculum learning and RL optimization.\\nCurriculum-based approaches, such as Light-R1 (Wen et al., 2025) and FASTCURL (Song et al., 2025),\\nprogressively increase task complexity to facilitate stable learning. Light-R1 employs curriculum SFT and\\nmulti-stage post-training, achieving strong performance with public datasets. FASTCURL extends this idea\\nby combining curriculum RL with progressive context window extension, enabling efficient training of R1-\\nlike models even on limited hardware. On the RL front, DAPO (Yu et al., 2025b) proposes a scalable and\\nopen-source RL system, leveraging decoupled clipping and dynamic sampling for improved training stability.\\nAGPO (Li et al., 2025a) addresses critical instability in the popular GRPO (Guo et al., 2025) by introducing\\na revised advantage estimation that mitigates zero-variance issues. Some coreset methods focus on reduc-\\ning the quantity of training data. LIMO (Ye et al., 2025) argues that complex reasoning abilities are not\\nlearned from scratch but elicited through high-quality samples. By constructing a carefully curated dataset\\nof only 817 reasoning samples, the model trained on this data significantly outperforms those trained on\\nnearly 100K examples. The dataset construction involves filtering out easy problems, retaining challenging\\nones where advanced models struggle, and performing diversity-based sampling. Similarly, s1 (Muennighoff\\net al., 2025) constructs a compact dataset of 1,000 examples by jointly optimizing for difficulty, diversity, and\\nquality. Improving training efficiency through algorithmic innovations or data-centric approaches remains a\\npromising direction with substantial room for further exploration.\\nOpportunities in Traditional Model Compression.\\nTraditional model compression techniques offer\\nvaluable opportunities for improving reasoning efficiency. Among them, distillation has demonstrated sig-\\nnificant potential in enhancing reasoning efficiency. Distillation effectively transfers reasoning abilities from\\nlarger models to smaller ones, enabling them to achieve strong reasoning while significantly reducing costs\\n(see Section 3.2.1). Chen et al. (2025b) systematically investigates three key factors that influence the effec-\\ntiveness of CoT distillation: the granularity of reasoning paths, the format in which reasoning is presented,\\nand the choice of teacher model. These insights offer practical guidance for advancing the distillation of\\nreasoning abilities in small language models. Furthermore, distillation can play a role in other efficient rea-\\nsoning directions, such as latent reasoning, where it helps compress explicit CoTs into more compact implicit\\nreasoning paths (see Section 3.1.4) and SFT with variable-length CoT data (see Section 3.1.2). Distillation\\nis a promising strategy for efficient reasoning, though there remains room for improvement. Additionally, en-\\nhancing the efficiency of the distillation process itself is also a valuable direction for future research. Beyond\\ndistillation, other model compression techniques, such as quantization and pruning, also show potential.\\n17\\nPublished in Transactions on Machine Learning Research (09/2025)\\nAlthough preliminary pruning experiments were not promising, successful quantization suggests that model\\ncompression can maintain reasoning performance while improving efficiency in areas like memory usage.\\nAdvancing Sustainability through Efficient Reasoning.\\nAs discussed in this work, efficient reasoning\\ntechniques contribute to optimizing the efficiency of reasoning models, reducing computational costs, and\\nminimizing resource usage.\\nThese approaches help reduce the carbon footprint by lowering the energy\\nrequirements and supporting more environmentally friendly practices. As the use of reasoning models grows,\\nadopting more efficient methods can play a crucial role in mitigating the environmental impact. Additionally,\\nthese efficiency improvements do not introduce significant negative effects, ensuring the benefits are realized\\nwithout unintended consequences.\\nComparison with Related Surveys.\\nSeveral recent surveys have discussed reasoning models from dif-\\nferent angles. For example, Towards Reasoning Era (Chen et al., 2025a) provides a comprehensive overview\\nof long CoT reasoning, focusing primarily on reasoning performance and structure, but does not empha-\\nsize efficiency as a central concern. Some surveys (Qu et al., 2025a; Sui et al., 2025b) center on reasoning\\nefficiency. The former (Qu et al., 2025a) organizes methods by stages in the LLM development lifecycle\\n(e.g., pre-training, supervised fine-tuning, reinforcement learning, and inference), offering a broad perspec-\\ntive across the modeling pipeline. The latter (Sui et al., 2025b) classifies approaches based on their core\\ntechnical mechanisms (e.g., model-based, output-based, and prompt-based), clearly distinguishing the un-\\nderlying methodological paths. In contrast, our work focuses on how efficiency is achieved during reasoning\\nitself, offering a goal-driven taxonomy centered around making reasoning shorter, smaller, and faster. This\\nstructured perspective helps clarify the design space of efficient reasoning and provides clearer guidance for\\nfuture research.\\nConnection between Intrinsic Efficiency Metrics and Hard Performance Metrics.\\nIn practical\\napplications, users are primarily concerned with the efficiency that reasoning methods bring to model de-\\nployment and usage, typically measured by hard performance metrics such as time and memory. However,\\nefficient reasoning methods often report token count rather than actual runtime. In practice, token count\\nand latency are strongly correlated. We empirically validated this on Qwen2.5-7B using the MAHT-500\\ndataset, where we observed a clear positive correlation between token count and latency. The Pearson corre-\\nlation coefficient was 0.9998 with a near-zero p-value, indicating a statistically significant and nearly perfect\\nlinear relationship. Meanwhile, some efficient reasoning methods employ PEFT techniques, such as LoRA,\\nto reduce memory usage and calculation costs during the SFT or RL stages. However, this reduction applies\\nonly to the training stage and does not affect memory usage during inference or downstream deployment.\\n6\\nConclusion\\nIn conclusion, this survey provides a comprehensive overview of efficient reasoning techniques. We cate-\\ngorize current efforts into three main directions—shorter, smaller, and faster—each addressing reasoning\\nefficiency from a unique perspective: compressing reasoning chains, building small language models with\\nstrong reasoning abilities, and accelerating the decoding stage. As reasoning efficiency continues to gain\\ntraction, we believe it holds significant promise for enabling scalable and practical deployment of reasoning\\nmodels across diverse applications, from real-time systems to resource-constrained environments. We hope\\nthis survey serves as a valuable foundation for future research and development in this critical and rapidly\\nevolving field.\\nAcknowledgments\\nThis project is supported by the Ministry of Education, Singapore, under its Academic Research Fund Tier\\n2 (Award Number: MOE-T2EP20122-0006).\\n18\\nPublished in Transactions on Machine Learning Research (09/2025)\\nReferences\\nPranjal Aggarwal and Sean Welleck. L1: Controlling how long a reasoning model thinks with reinforcement\\nlearning. arXiv preprint arXiv:2503.04697, 2025.\\nPranjal Aggarwal, Aman Madaan, Yiming Yang, et al. Let’s sample step by step: Adaptive-consistency for\\nefficient reasoning and coding with llms. arXiv preprint arXiv:2305.11860, 2023.\\nOpen AI. Introducing openai o1-preview. 2024.\\nLasse F Wolff Anthony, Benjamin Kanding, and Raghavendra Selvan. Carbontracker: Tracking and predict-\\ning the carbon footprint of training deep learning models. arXiv preprint arXiv:2007.03051, 2020.\\nAnthropic. Claude 3.7 sonnet. 2025.\\nDaman Arora and Andrea Zanette.\\nTraining language models to reason efficiently.\\narXiv preprint\\narXiv:2502.04463, 2025.\\nBerk Atil, Alexa Chittams, Liseng Fu, Ferhan Ture, Lixinyu Xu, and Breck Baldwin. Llm stability: A\\ndetailed analysis with some surprises. arXiv preprint arXiv:2408.04667, 2024.\\nSimon A Aytes, Jinheon Baek, and Sung Ju Hwang. Sketch-of-thought: Efficient llm reasoning with adaptive\\ncognitive-inspired sketching. arXiv preprint arXiv:2503.05179, 2025.\\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna\\nGajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate\\nproblems with large language models. In AAAI, 2024.\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan,\\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models\\ntrained on code. arXiv preprint arXiv:2107.03374, 2021.\\nQiguang Chen, Libo Qin, Jiaqi Wang, Jingxuan Zhou, and Wanxiang Che. Unlocking the capabilities of\\nthought: A reasoning boundary framework to quantify and optimize chain-of-thought. In NeurIPS, 2024a.\\nQiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang\\nZhou, Te Gao, and Wanxiang Che. Towards reasoning era: A survey of long chain-of-thought for reasoning\\nlarge language models. arXiv preprint arXiv:2503.09567, 2025a.\\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Dis-\\nentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588,\\n2022.\\nXiaoshu Chen, Sihang Zhou, Ke Liang, and Xinwang Liu. Distilling reasoning ability from large language\\nmodels with adaptive thinking. arXiv preprint arXiv:2404.09170, 2024b.\\nXinghao Chen, Zhijing Sun, Wenjin Guo, Miaoran Zhang, Yanjun Chen, Yirong Sun, Hui Su, Yijie Pan,\\nDietrich Klakow, Wenjie Li, et al. Unveiling the key factors for distilling chain-of-thought reasoning. arXiv\\npreprint arXiv:2502.18001, 2025b.\\nXingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei\\nZhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms.\\narXiv preprint arXiv:2412.21187, 2024c.\\nXinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-\\ndebug. In ICLR, 2024d.\\nJeffrey Cheng and Benjamin Van Durme. Compressed chain of thought: Efficient reasoning through dense\\nrepresentations. arXiv preprint arXiv:2412.13171, 2024.\\n19\\nPublished in Transactions on Machine Learning Research (09/2025)\\nYu-Neng Chuang, Helen Zhou, Prathusha Sarma, Parikshit Gopalan, John Boccio, Sara Bolouki, and Xia\\nHu. Learning to route llms with confidence tokens. arXiv preprint arXiv:2410.13284, 2024.\\nYu-Neng Chuang, Leisheng Yu, Guanchu Wang, Lizhe Zhang, Zirui Liu, Xuanting Cai, Yang Sui, Vladimir\\nBraverman, and Xia Hu. Confident or seek stronger: Exploring uncertainty-based on-device llm routing\\nfrom benchmarking to generalization. arXiv preprint arXiv:2502.04428, 2025.\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word\\nproblems. arXiv preprint arXiv:2110.14168, 2021.\\nRémi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In International con-\\nference on computers and games, 2006.\\nAlejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu,\\nLuis Gaspar Schroeder, Tian Xia, Huanzhi Mao, et al.\\nThe danger of overthinking: Examining the\\nreasoning-action dilemma in agentic tasks. arXiv preprint arXiv:2502.08235, 2025.\\nCan Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zichong\\nYang, Kuei-Da Liao, et al. A survey on multimodal large language models for autonomous driving. In\\nProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2024.\\nYingqian Cui, Pengfei He, Jingying Zeng, Hui Liu, Xianfeng Tang, Zhenwei Dai, Yan Han, Chen Luo, Jing\\nHuang, Zhen Li, et al. Stepwise perplexity-guided refinement for efficient chain-of-thought reasoning in\\nlarge language models. arXiv preprint arXiv:2502.13260, 2025.\\nQuy-Anh Dang and Chris Ngo. Reinforcement learning for reasoning in small llms: What works and what\\ndoesn’t. arXiv preprint arXiv:2503.16219, 2025.\\nYuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber.\\nImplicit chain of thought reasoning via knowledge distillation. arXiv preprint arXiv:2311.01460, 2023.\\nYuntian Deng, Yejin Choi, and Stuart Shieber. From explicit cot to implicit cot: Learning to internalize cot\\nstep by step. arXiv preprint arXiv:2405.14838, 2024.\\nMengru Ding, Hanmeng Liu, Zhizhang Fu, Jian Song, Wenbo Xie, and Yue Zhang. Break the chain: Large\\nlanguage models can be shortcut reasoners. arXiv preprint arXiv:2406.06580, 2024.\\nRuomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan,\\nQingwei Lin, and Dongmei Zhang. Everything of thoughts: Defying the law of penrose triangle for thought\\ngeneration. arXiv preprint arXiv:2311.04254, 2023.\\nYifu Ding, Wentao Jiang, Shunyu Liu, Yongcheng Jing, Jinyang Guo, Yingjie Wang, Jing Zhang, Zengmao\\nWang, Ziwei Liu, Bo Du, et al. Dynamic parallel tree search for efficient llm reasoning. arXiv preprint\\narXiv:2502.16235, 2025.\\nJiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, and Cheston Tan. A survey of embodied ai: From\\nsimulators to research tasks. IEEE Transactions on Emerging Topics in Computational Intelligence, 6(2):\\n230–244, 2022.\\nGongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and Xinchao Wang.\\nDepgraph: Towards any\\nstructural pruning. In CVPR, 2023.\\nGongfan Fang, Xinyin Ma, Michael Bi Mi, and Xinchao Wang. Isomorphic pruning for vision models. In\\nECCV, 2024.\\nGongfan Fang, Xinyin Ma, and Xinchao Wang.\\nThinkless: Llm learns when to think.\\narXiv preprint\\narXiv:2505.13379, 2025.\\n20\\nPublished in Transactions on Machine Learning Research (09/2025)\\nSicheng Feng, Siyu Li, Luonan Chen, and Shengquan Chen. Unveiling potential threats: backdoor attacks\\nin single-cell pre-trained models. Cell Discovery, 10(1):122, 2024a.\\nSicheng Feng, Keda Tao, and Huan Wang.\\nIs oracle pruning the true oracle?\\narXiv preprint\\narXiv:2412.00143, 2024b.\\nSicheng Feng, Song Wang, Shuyi Ouyang, Lingdong Kong, Zikai Song, Jianke Zhu, Huan Wang, and Xinchao\\nWang. Can mllms guide me home? a benchmark study on fine-grained visual reasoning from transit maps.\\narXiv preprint arXiv:2505.18675, 2025.\\nTao Feng, Yicheng Li, Li Chenglin, Hao Chen, Fei Yu, and Yin Zhang. Teaching small language models\\nreasoning through counterfactual distillation. In EMNLP, 2024c.\\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training compres-\\nsion for generative pretrained transformers. In ICLR, 2023a.\\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization\\nfor generative pre-trained transformers. In ICLR, 2023b.\\nPeizhong Gao, Ao Xie, Shaoguang Mao, Wenshan Wu, Yan Xia, Haipeng Mi, and Furu Wei. Meta reasoning\\nfor large language models. arXiv preprint arXiv:2406.11698, 2024.\\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant.\\nDid aristotle\\nuse a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the\\nAssociation for Computational Linguistics, 2021.\\nAngeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopou-\\nlos. Looped transformers as programmable computers. In ICML, 2023.\\nVinod Goel. Sketches of thought. MIT press, 1995.\\nZhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic:\\nLarge language models can self-correct with tool-interactive critiquing. In ICLR, 2024.\\nRobert M. Gray and David L. Neuhoff. Quantization. IEEE transactions on information theory, 1998.\\nAlbert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In COLM,\\n2024.\\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,\\nPeiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\\nlearning. arXiv preprint arXiv:2501.12948, 2025.\\nSong Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with\\npruning, trained quantization and huffman coding. In ICLR, 2016.\\nTingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. Token-budget-\\naware llm reasoning. arXiv preprint arXiv:2412.18547, 2024.\\nJitai Hao, Yuke Zhu, Tian Wang, Jun Yu, Xin Xin, Bo Zheng, Zhaochun Ren, and Sheng Guo. Omnikv:\\nDynamic context selection for efficient long-context llms. In ICLR, 2025.\\nShibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian.\\nTraining large language models to reason in a continuous latent space. arXiv preprint arXiv:2412.06769,\\n2024.\\nMasoud Hashemi, Oluwanifemi Bamgbose, Sathwik Tejaswi Madhusudhan, Jishnu Sethumadhavan Nair,\\nAman Tiwari, and Vikas Yadav. Dna bench: When silence is smarter–benchmarking over-reasoning in\\nreasoning llms. arXiv preprint arXiv:2503.15793, 2025.\\n21\\nPublished in Transactions on Machine Learning Research (09/2025)\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and\\nJacob Steinhardt.\\nMeasuring mathematical problem solving with the math dataset.\\narXiv preprint\\narXiv:2103.03874, 2021.\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint\\narXiv:1503.02531, 2015.\\nBairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang. Thinkprune:\\nPruning long chain-of-thought of llms via reinforcement learning. arXiv preprint arXiv:2504.01296, 2025.\\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu\\nChen, et al. Lora: Low-rank adaptation of large language models. In ICLR, 2022.\\nHanxu Hu, Hongyuan Lu, Huajian Zhang, Yun-Ze Song, Wai Lam, and Yue Zhang. Chain-of-symbol prompt-\\ning for spatial reasoning in large language models. In First Conference on Language Modeling, 2024.\\nYangliu Hu, Zikai Song, Na Feng, Yawei Luo, Junqing Yu, Yi-Ping Phoebe Chen, and Wei Yang.\\nSf2t: Self-supervised fragment finetuning of video-llms for fine-grained understanding.\\narXiv preprint\\narXiv:2504.07745, 2025.\\nChengsong Huang, Langlin Huang, Jixuan Leng, Jiacheng Liu, and Jiaxin Huang. Efficient test-time scaling\\nvia self-calibration. arXiv preprint arXiv:2503.00031, 2025.\\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Alek-\\nsander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720,\\n2024.\\nMingyu Jin, Weidi Luo, Sitao Cheng, Xinyi Wang, Wenyue Hua, Ruixiang Tang, William Yang Wang, and\\nYongfeng Zhang. Disentangling memory and reasoning ability in large language models. arXiv preprint\\narXiv:2411.13504, 2024a.\\nMingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, and Mengnan\\nDu. The impact of reasoning step length on large language models. arXiv preprint arXiv:2401.04925,\\n2024b.\\nShuowei Jin, Yongji Wu, Haizhong Zheng, Qingzhao Zhang, Matthew Lentz, Z Morley Mao, Atul Prakash,\\nFeng Qian, and Danyang Zhuo. Adaptive skeleton graph decoding. arXiv preprint arXiv:2402.12280,\\n2024c.\\nYu Kang, Xianghui Sun, Liangyu Chen, and Wei Zou. C3ot: Generating shorter chain-of-thought without\\ncompromising effectiveness. arXiv preprint arXiv:2412.11664, 2024.\\nAybora Koksal and Aydin Alatan Alatan. Milchat: Introducing chain of thought reasoning and grpo to a\\nmultimodal small language model for remote sensing. arXiv preprint arXiv:2505.07984, 2025.\\nMartin Kuo, Jianyi Zhang, Aolin Ding, Qinsi Wang, Louis DiValentin, Yujia Bao, Wei Wei, Da-Cheng Juan,\\nHai Li, and Yiran Chen. H-cot: Hijacking the chain-of-thought safety reasoning mechanism to jailbreak\\nlarge reasoning models, including openai o1/o3, deepseek-r1, and gemini 2.0 flash thinking. arXiv preprint\\narXiv:2502.12893, 2025.\\nAlexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the carbon\\nemissions of machine learning. arXiv preprint arXiv:1910.09700, 2019.\\nYann LeCun, John Denker, and Sara Solla. Optimal brain damage. In NeurIPS, 1989.\\nAyeong Lee, Ethan Che, and Tianyi Peng. How well do llms compress their own chain-of-thought? a token\\ncomplexity approach. arXiv preprint arXiv:2503.01141, 2025.\\nChen Li, Nazhou Liu, and Kai Yang. Adaptive group policy optimization: Towards stable training and\\ntoken-efficient reasoning. arXiv preprint arXiv:2503.15952, 2025a.\\n22\\nPublished in Transactions on Machine Learning Research (09/2025)\\nChenglin Li, Qianglong Chen, Liangyue Li, Caiyu Wang, Yicheng Li, Zulong Chen, and Yin Zhang. Mixed\\ndistillation helps smaller language model better reasoning. arXiv preprint arXiv:2312.10730, 2023a.\\nPeiji Li, Kai Lv, Yunfan Shao, Yichuan Ma, Linyang Li, Xiaoqing Zheng, Xipeng Qiu, and Qipeng Guo.\\nFastmcts: A simple sampling strategy for data synthesis. arXiv preprint arXiv:2502.11476, 2025b.\\nWentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, and Lei Zhang. Token-\\npacker: Efficient visual projector for multimodal llm. In IJCV, 2025c.\\nXuying Li, Zhuo Li, Yuji Kosuga, and Victor Bian. Output length effect on deepseek-r1’s safety in forced\\nthinking. arXiv preprint arXiv:2503.01923, 2025d.\\nYiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Bin Sun, Xinglin Wang, Heda Wang, and Kan Li.\\nTurning dust into gold: Distilling complex reasoning capabilities from llms by leveraging negative data.\\nIn AAAI, 2024a.\\nYiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Xinglin Wang, Bin Sun, Heda Wang, and Kan\\nLi.\\nEscape sky-high cost:\\nEarly-stopping self-consistency for multi-step reasoning.\\narXiv preprint\\narXiv:2401.10480, 2024b.\\nYuetai Li, Xiang Yue, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Bhaskar Ramasubra-\\nmanian, and Radha Poovendran. Small models struggle to learn from strong reasoners. arXiv preprint\\narXiv:2502.12143, 2025e.\\nYun Li, Lin Niu, Xipeng Zhang, Kai Liu, Jianchen Zhu, and Zhanhui Kang. E-sparse: Boosting the large\\nlanguage model inference through entropy-based n: M sparsity. arXiv preprint arXiv:2310.15929, 2023b.\\nBaohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sahoo, and Caiming\\nXiong. Reward-guided speculative decoding for efficient llm reasoning. arXiv preprint arXiv:2501.19324,\\n2025a.\\nHuanxuan Liao, Shizhu He, Yupu Hao, Xiang Li, Yuanzhe Zhang, Jun Zhao, and Kang Liu.\\nSkintern:\\nInternalizing symbolic knowledge for distilling better cot capabilities into small language models.\\nIn\\nCOLING, 2025b.\\nJonathan Light, Wei Cheng, Wu Yue, Masafumi Oyamada, Mengdi Wang, Santiago Paternain, and Haifeng\\nChen. Disc: Dynamic decomposition improves llm inference scaling. arXiv preprint arXiv:2502.16706,\\n2025.\\nHunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike,\\nJohn Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In ICLR, 2023.\\nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao,\\nXingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for llm compression\\nand acceleration. In MLSys, 2024.\\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation:\\nLearning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146, 2017.\\nFan Liu, Wenshuo Chao, Naiqiang Tan, and Hao Liu. Bag of tricks for inference-time computation of llm\\nreasoning. arXiv preprint arXiv:2502.07191, 2025a.\\nJinyi Liu, Yan Zheng, Rong Cheng, Qiyu Wu, Wei Guo, Fei Ni, Hebin Liang, Yifu Yuan, Hangyu Mao,\\nFuzheng Zhang, et al. From chaos to order: The atomic reasoner framework for fine-grained reasoning in\\nlarge language models. arXiv preprint arXiv:2503.15944, 2025b.\\nJunnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu, Songyang Gao, Wenwei Zhang, Songyang\\nZhang, and Kai Chen. Are your llms capable of stable reasoning? arXiv preprint arXiv:2412.13147, 2024a.\\n23\\nPublished in Transactions on Machine Learning Research (09/2025)\\nRuikang Liu, Yuxuan Sun, Manyi Zhang, Haoli Bai, Xianzhi Yu, Tiezheng Yu, Chun Yuan, and Lu Hou.\\nQuantization hurts reasoning?\\nan empirical study on quantized reasoning models.\\narXiv preprint\\narXiv:2504.04823, 2025c.\\nRunze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. Can\\n1b llm surpass 405b llm? rethinking compute-optimal test-time scaling. arXiv preprint arXiv:2502.06703,\\n2025d.\\nTengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng Jiayang, Yue Zhang, Xipeng Qiu, and Zheng Zhang. Can\\nlanguage models learn to skip steps? arXiv preprint arXiv:2411.01855, 2024b.\\nTianqiao Liu, Zui Chen, Zitao Liu, Mi Tian, and Weiqi Luo. Expediting and elevating large language model\\nreasoning via hidden chain-of-thought decoding. arXiv preprint arXiv:2409.08561, 2024c.\\nYufan Liu, Jiajiong Cao, Bing Li, Chunfeng Yuan, Weiming Hu, Yangxi Li, and Yunqiang Duan. Knowledge\\ndistillation via instance relationship graph. In CVPR, 2019.\\nEnzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, En-\\nming Yuan, Yuzhi Wang, et al. Moba: Mixture of block attention for long-context llms. arXiv preprint\\narXiv:2502.13189, 2025.\\nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin,\\nShifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language\\nmodels via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.\\nHaotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and\\nDacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. arXiv preprint\\narXiv:2501.12570, 2025a.\\nMichael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y Tang, Manan Roongta, Colin Cai, Jeffrey\\nLuo, Tianjun Zhang, Li Erran Li, et al. Deepscaler: Surpassing o1-preview with a 1.5 b model by scaling\\nrl. Notion Blog, 2025b.\\nYijia Luo, Yulin Song, Xingyao Zhang, Jiaheng Liu, Weixun Wang, GengRu Chen, Wenbo Su, and Bo Zheng.\\nDeconstructing long chain-of-thought: A structured reasoning optimization framework for long cot distil-\\nlation. arXiv preprint arXiv:2503.16385, 2025c.\\nChang Ma, Haiteng Zhao, Junlei Zhang, Junxian He, and Lingpeng Kong. Non-myopic generation of language\\nmodels for reasoning and planning. arXiv preprint arXiv:2410.17195, 2024.\\nXinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language\\nmodels. In NeurIPS, 2023.\\nXinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang.\\nCot-valve:\\nLength-\\ncompressible chain-of-thought tuning. arXiv preprint arXiv:2502.09601, 2025.\\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha\\nDziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. In\\nNeurIPS, 2023.\\nLucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. Teaching\\nsmall language models to reason. arXiv preprint arXiv:2212.08410, 2022.\\nEthan Mendes and Alan Ritter. Language models can self-improve at state-value estimation for better search.\\narXiv preprint arXiv:2503.02878, 2025.\\nNiklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettle-\\nmoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv\\npreprint arXiv:2501.19393, 2025.\\n24\\nPublished in Transactions on Machine Learning Research (09/2025)\\nTergel Munkhbat, Namgyu Ho, Seohyun Kim, Yongjin Yang, Yujin Kim, and Se-Young Yun. Self-training\\nelicits concise reasoning in large language models. arXiv preprint arXiv:2502.20122, 2025.\\nXuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang, Huazhong Yang, and Yu Wang. Skeleton-of-thought:\\nPrompting llms for efficient parallel generation. arXiv preprint arXiv:2307.15337, 2023.\\nIsaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph E Gonzalez, M Waleed\\nKadous, and Ion Stoica.\\nRoutellm:\\nLearning to route llms with preference data.\\narXiv preprint\\narXiv:2406.18665, 2024.\\nOpenAI. OpenAI o1. https://openai.com/o1/, 2024.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\\nhuman feedback. In NeurIPS, 2022.\\nShuyi Ouyang, Hongyi Wang, Shiao Xie, Ziwei Niu, Ruofeng Tong, Yen-Wei Chen, and Lanfen Lin. Slvit:\\nScale-wise language-guided vision transformer for referring image segmentation. In IJCAI, 2023.\\nDaniele Paliotta, Junxiong Wang, Matteo Pagliardini, Kevin Y Li, Aviv Bick, J Zico Kolter, Albert Gu,\\nFrançois Fleuret, and Tri Dao. Thinking slow, fast: Scaling inference compute with distilled reasoners.\\narXiv preprint arXiv:2502.20339, 2025.\\nRui Pan, Yinwei Dai, Zhihao Zhang, Gabriele Oliaro, Zhihao Jia, and Ravi Netravali. Specreason: Fast and\\naccurate inference-time compute via speculative reasoning. arXiv preprint arXiv:2504.07891, 2025.\\nShubham Parashar, Blake Olson, Sambhav Khurana, Eric Li, Hongyi Ling, James Caverlee, and Shuiwang\\nJi. Inference-time computations for llm reasoning and planning: A benchmark and insights. arXiv preprint\\narXiv:2502.12521, 2025.\\nJacob Pfau, William Merrill, and Samuel R Bowman.\\nLet’s think dot by dot: Hidden computation in\\ntransformer language models. In COLM, 2024.\\nS Joe Qin and Thomas A Badgwell. An overview of industrial model predictive control technology. In AIche\\nsymposium series, 1997.\\nXiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong Liu, Shuxian\\nLiang, Junxian He, et al. A survey of efficient reasoning for large reasoning models: Language, multi-\\nmodality, and beyond. arXiv preprint arXiv:2503.21614, 2025a.\\nYuxiao Qu, Matthew YR Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching, Ruslan Salakhut-\\ndinov, and Aviral Kumar. Optimizing test-time compute via meta reinforcement fine-tuning. arXiv preprint\\narXiv:2503.07572, 2025b.\\nMatthew Renze and Erhan Guven. The benefits of a concise chain of thought on problem-solving in large\\nlanguage models. In FLLM, 2024.\\nAbulhair Saparov and He He.\\nLanguage models are greedy reasoners: A systematic formal analysis of\\nchain-of-thought. In ICLR, 2023.\\nNikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, and Sashank J Reddi. Reasoning with latent\\nthoughts: On the power of looped transformers. In ICLR, 2025.\\nVictor Schmidt, Kamal Goyal, Aditya Joshi, Boris Feld, Liam Conell, Nikolas Laskaris, Doug Blank,\\nJonathan Wilson, Sorelle Friedler, and Sasha Luccioni. Codecarbon: estimate and track carbon emis-\\nsions from machine learning computing (2021). DOI: https://doi. org/10.5281/zenodo, 4658424, 2021.\\nKele Shao, Keda Tao, Kejia Zhang, Sicheng Feng, Mu Cai, Yuzhang Shang, Haoxuan You, Can Qin, Yang\\nSui, and Huan Wang. When tokens talk too much: A survey of multimodal long-context token compression\\nacross images, videos, and audios. arXiv preprint arXiv:2507.20198, 2025.\\n25\\nPublished in Transactions on Machine Learning Research (09/2025)\\nXuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, and Jiuxiang Gu. Efficient reasoning with\\nhidden thinking. arXiv preprint arXiv:2501.19201, 2025a.\\nYi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang,\\nand Shiguo Lian.\\nDast: Difficulty-adaptive slow-thinking for large reasoning models.\\narXiv preprint\\narXiv:2503.04472, 2025b.\\nZhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, and Yulan He. Codi: Compressing chain-\\nof-thought into continuous space via self-distillation. arXiv preprint arXiv:2502.21074, 2025c.\\nCharlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be\\nmore effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024.\\nMingyang Song, Mao Zheng, Zheng Li, Wenjie Yang, Xuan Luo, Yue Pan, and Feng Zhang.\\nFastcurl:\\nCurriculum reinforcement learning with progressive context extension for efficient training r1-like reasoning\\nmodels. arXiv preprint arXiv:2503.17287, 2025.\\nZayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal,\\nXinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Durrett. To cot or not to cot? chain-of-thought helps\\nmainly on math and symbolic reasoning. arXiv preprint arXiv:2409.12183, 2024.\\nGaurav Srivastava, Shuxiang Cao, and Xuan Wang. Towards reasoning ability of small language models.\\narXiv preprint arXiv:2502.11569, 2025.\\nDiJia Su, Hanlin Zhu, Yingchen Xu, Jiantao Jiao, Yuandong Tian, and Qinqing Zheng. Token assorted:\\nMixing latent and text tokens for improved language model reasoning. arXiv preprint arXiv:2502.03275,\\n2025.\\nYang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew\\nWen, Hanjie Chen, Xia Hu, et al. Stop overthinking: A survey on efficient reasoning for large language\\nmodels. arXiv preprint arXiv:2503.16419, 2025a.\\nYang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew\\nWen, Shaochen Zhong, Hanjie Chen, and Xia Hu. Stop overthinking: A survey on efficient reasoning for\\nlarge language models. arXiv preprint arXiv:2503.16419, 2025b.\\nYuan Sui, Yufei He, Tri Cao, Simeng Han, and Bryan Hooi. Meta-reasoner: Dynamic guidance for optimized\\ninference-time reasoning in large language models. arXiv preprint arXiv:2502.19918, 2025c.\\nHanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett,\\nand Andrea Zanette. Fast best-of-n decoding via speculative rejection. In NeurIPS, 2024a.\\nYutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, and Furu\\nWei. Multimodal latent language modeling with next-token diffusion. arXiv preprint arXiv:2412.08635,\\n2024b.\\nRichard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 1988.\\nWenhui Tan, Jiaze Li, Jianzhong Ju, Zhenbo Luo, Jian Luan, and Ruihua Song. Think silently, think fast:\\nDynamic latent compression of llm reasoning chains. arXiv preprint arXiv:2505.16552, 2025.\\nAmir Taubenfeld, Tom Sheffer, Eran Ofek, Amir Feder, Ariel Goldstein, Zorik Gekhman, and Gal Yona.\\nConfidence improves self-consistency in llms. arXiv preprint arXiv:2502.06233, 2025.\\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao,\\nChenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv\\npreprint arXiv:2501.12599, 2025.\\nFengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, and Yuyu Luo. Atom of thoughts for\\nmarkov llm test-time scaling. arXiv preprint arXiv:2502.12018, 2025.\\n26\\nPublished in Transactions on Machine Learning Research (09/2025)\\nKaiwen Tuo and Huan Wang. Sparsessm: Efficient selective structured state space models can be pruned in\\none-shot. arXiv preprint arXiv:2506.09613, 2025.\\nKarthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the planning\\nabilities of large language models-a critical investigation. In NeurIPS, 2023.\\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurIPS, 2017.\\nGuangya Wan, Yuqi Wu, Jie Chen, and Sheng Li. Reasoning aware self-consistency: Leveraging reasoning\\npaths for efficient llm sampling. arXiv preprint arXiv:2408.17017, 2024.\\nAnte Wang, Linfeng Song, Ye Tian, Dian Yu, Haitao Mi, Xiangyu Duan, Zhaopeng Tu, Jinsong Su, and\\nDong Yu. Don’t get lost in the trees: Streamlining llm reasoning by overcoming tree search exploration\\npitfalls. arXiv preprint arXiv:2502.11183, 2025a.\\nHuan Wang, Can Qin, Yulun Zhang, and Yun Fu. Neural pruning via growing regularization. In ICLR,\\n2021.\\nJunxiong Wang, Wen-Ding Li, Daniele Paliotta, Daniel Ritter, Alexander M Rush, and Tri Dao. M1: Towards\\nscalable test-time compute with mamba reasoning models. arXiv preprint arXiv:2504.10449, 2025b.\\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,\\nXu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. Frontiers of\\nComputer Science, 18(6):186345, 2024a.\\nSong Wang, Gongfan Fang, Lingdong Kong, Xiangtai Li, Jianyun Xu, Sheng Yang, Qiang Li, Jianke Zhu, and\\nXinchao Wang. Pixelthink: Towards efficient chain-of-pixel reasoning. arXiv preprint arXiv:2505.23727,\\n2025c.\\nXinglin Wang, Shaoxiong Feng, Yiwei Li, Peiwen Yuan, Yueqi Zhang, Chuyi Tan, Boyuan Pan, Yao Hu, and\\nKan Li. Make every penny count: Difficulty-adaptive self-consistency for cost-efficient reasoning. arXiv\\npreprint arXiv:2408.13457, 2024b.\\nXinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang Wang, and Alessandro Sordoni.\\nGuiding language model reasoning with planning tokens. In COLM, 2024c.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and\\nDenny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint\\narXiv:2203.11171, 2022a.\\nYiming Wang, Pei Zhang, Siyuan Huang, Baosong Yang, Zhuosheng Zhang, Fei Huang, and Rui Wang.\\nSampling-efficient test-time scaling:\\nSelf-estimating the best-of-n sampling in early decoding.\\narXiv\\npreprint arXiv:2503.01422, 2025d.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Han-\\nnaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint\\narXiv:2212.10560, 2022b.\\nYue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao\\nLi, Zhuosheng Zhang, et al. Thoughts are all over the place: On the underthinking of o1-like llms. arXiv\\npreprint arXiv:2501.18585, 2025e.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\\net al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022.\\nLiang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang,\\nXiaowei Lv, et al. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv\\npreprint arXiv:2503.10460, 2025.\\n27\\nPublished in Transactions on Machine Learning Research (09/2025)\\nHan Wu, Yuxuan Yao, Shuqi Liu, Zehua Liu, Xiaojin Fu, Xiongwei Han, Xing Li, Hui-Ling Zhen, Tao Zhong,\\nand Mingxuan Yuan. Unlocking efficient long-to-short llm reasoning with model merging. arXiv preprint\\narXiv:2503.20641, 2025a.\\nSiye Wu, Jian Xie, Yikai Zhang, Aili Chen, Kai Zhang, Yu Su, and Yanghua Xiao. Arm: Adaptive reasoning\\nmodel. arXiv preprint arXiv:2505.20258, 2025b.\\nYangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical\\nanalysis of compute-optimal inference for problem-solving with language models. In ICLR, 2025c.\\nYuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, and Yisen Wang. When more is less: Understanding\\nchain-of-thought length in llms. arXiv preprint arXiv:2502.07266, 2025d.\\nHeming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, and Wenjie Li. Tokenskip: Controllable chain-of-\\nthought compression in llms. arXiv preprint arXiv:2502.12067, 2025.\\nKun Xiang, Zhili Liu, Zihao Jiang, Yunshuang Nie, Kaixin Cai, Yiyang Yin, Runhui Huang, Haoxiang Fan,\\nHanhui Li, Weiran Huang, Yihan Zeng, Yu-Jie Yuan, Jianhua Han, Lanqing Hong, Hang Xu, and Xiaodan\\nLiang. Can atomic step decomposition enhance the self-structured reasoning of multimodal large models?\\narXiv preprint arXiv:2503.06252, 2025a.\\nKun Xiang, Zhili Liu, Zihao Jiang, Yunshuang Nie, Kaixin Cai, Yiyang Yin, Runhui Huang, Haoxiang Fan,\\nHanhui Li, Weiran Huang, et al. Can atomic step decomposition enhance the self-structured reasoning of\\nmultimodal large models? arXiv preprint arXiv:2503.06252, 2025b.\\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant: Accurate\\nand efficient post-training quantization for large language models. In ICML, 2023.\\nFangzhi Xu, Hang Yan, Chang Ma, Haiteng Zhao, Jun Liu, Qika Lin, and Zhiyong Wu.\\nϕ-decoding:\\nAdaptive foresight sampling for balanced inference-time exploration and exploitation.\\narXiv preprint\\narXiv:2503.13288, 2025a.\\nFengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui\\nGong, Tianjian Ouyang, Fanjin Meng, et al. Towards large reasoning models: A survey of reinforced\\nreasoning with large language models. arXiv preprint arXiv:2501.09686, 2025b.\\nSilei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. Chain of draft: Thinking faster by writing less.\\narXiv preprint arXiv:2502.18600, 2025c.\\nYige Xu, Xu Guo, Zhiwei Zeng, and Chunyan Miao. Softcot: Soft chain-of-thought for efficient reasoning\\nwith llms. arXiv preprint arXiv:2502.12134, 2025d.\\nYuchen Yan, Yongliang Shen, Yang Liu, Jin Jiang, Mengdi Zhang, Jian Shao, and Yueting Zhuang. In-\\nftythink: Breaking the length limits of long-context reasoning in large language models. arXiv preprint\\narXiv:2503.06692, 2025.\\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,\\nFei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024a.\\nChenxiao Yang, Nathan Srebro, David McAllester, and Zhiyuan Li.\\nPencil: Long thoughts with short\\nmemory. arXiv preprint arXiv:2503.14337, 2025a.\\nEnneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, and Dacheng Tao. Model\\nmerging in llms, mllms, and beyond: Methods, theories, applications and opportunities. arXiv preprint\\narXiv:2408.07666, 2024b.\\nJunjie Yang, Ke Lin, and Xing Yu. Think when you need: Self-adaptive chain-of-thought learning. arXiv\\npreprint arXiv:2504.03234, 2025b.\\n28\\nPublished in Transactions on Machine Learning Research (09/2025)\\nWen Yang, Minpeng Liao, and Kai Fan. Markov chain of thought for efficient mathematical reasoning. arXiv\\npreprint arXiv:2410.17635, 2024c.\\nWenkai Yang, Shuming Ma, Yankai Lin, and Furu Wei.\\nTowards thinking-optimal scaling of test-time\\ncompute for llm reasoning. arXiv preprint arXiv:2502.18080, 2025c.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and\\nChristopher D Manning.\\nHotpotqa: A dataset for diverse, explainable multi-hop question answering.\\narXiv preprint arXiv:1809.09600, 2018.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan.\\nTree of thoughts: Deliberate problem solving with large language models. In NeurIPS, 2023.\\nShunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ-bench: A benchmark for tool-agent-\\nuser interaction in real-world domains. arXiv preprint arXiv:2406.12045, 2024.\\nYixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu.\\nLimo: Less is more for\\nreasoning. arXiv preprint arXiv:2502.03387, 2025.\\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo\\nLi, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large\\nlanguage models. arXiv preprint arXiv:2309.12284, 2023.\\nPing Yu, Jing Xu, Jason Weston, and Ilia Kulikov.\\nDistilling system 2 into system 1.\\narXiv preprint\\narXiv:2407.06023, 2024.\\nQifan Yu, Zhenyu He, Sijie Li, Xun Zhou, Jun Zhang, Jingjing Xu, and Di He. Enhancing auto-regressive\\nchain-of-thought through loop-aligned reasoning. arXiv preprint arXiv:2502.08482, 2025a.\\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu,\\nLingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv\\npreprint arXiv:2503.14476, 2025b.\\nJingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX Wei,\\nLean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse\\nattention. arXiv preprint arXiv:2502.11089, 2025.\\nWeihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo:\\nInvestigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint\\narXiv:2503.18892, 2025a.\\nZhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Yunhua Zhou, and Xipeng Qiu.\\nRevisiting the test-\\ntime scaling of o1-like models: Do they truly possess test-time scaling capabilities?\\narXiv preprint\\narXiv:2502.12215, 2025b.\\nJintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng, Huajun Chen, and\\nNingyu Zhang. Lightthinker: Thinking step-by-step compression. arXiv preprint arXiv:2502.15589, 2025a.\\nNan Zhang, Yusen Zhang, Prasenjit Mitra, and Rui Zhang. When reasoning meets compression: Bench-\\nmarking compressed large reasoning models on complex reasoning tasks. arXiv preprint arXiv:2504.02010,\\n2025b.\\nYulun Zhang, Huan Wang, Can Qin, and Yun Fu. Learning efficient image super-resolution networks via\\nstructure-regularized pruning. In ICLR, 2021.\\nYunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee,\\nand Lu Wang.\\nSmall language models need strong verifiers to self-correct reasoning.\\narXiv preprint\\narXiv:2404.17140, 2024.\\n29\\nPublished in Transactions on Machine Learning Research (09/2025)\\nYichun Zhao, Shuheng Zhou, and Huijia Zhu.\\nProbe then retrieve and reason: Distilling probing and\\nreasoning capabilities into smaller language models. In LREC-COLING, 2024.\\nHuaixiu Steven Zheng, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou,\\nHeng-Tze Cheng, Quoc V Le, Ed H Chi, et al. Natural plan: Benchmarking llms on natural language\\nplanning. arXiv preprint arXiv:2406.04520, 2024.\\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire\\nCui, Olivier Bousquet, Quoc Le, et al.\\nLeast-to-most prompting enables complex reasoning in large\\nlanguage models. In ICLR, 2023.\\nZhi Zhou, Tan Yuhao, Zenan Li, Yuan Yao, Lan-Zhe Guo, Xiaoxing Ma, and Yu-Feng Li. Bridging internal\\nprobability and self-consistency for effective and efficient llm reasoning. arXiv preprint arXiv:2502.00511,\\n2025.\\nJiace Zhu, Yingtao Shen, Jie Zhao, and An Zou. Path-consistency: Prefix enhancement for efficient inference\\nin llm. arXiv preprint arXiv:2409.01281, 2024a.\\nXunyu Zhu, Jian Li, Can Ma, and Weiping Wang. Improving mathematical reasoning capabilities of small\\nlanguage models via feedback-driven distillation. arXiv preprint arXiv:2411.14698, 2024b.\\nA\\nAppendix\\nA.1\\nDetails for Model Compression\\nQuantization.\\nQuantization improves model efficiency and reduces memory usage by lowering the bit\\nprecision of parameters. It is typically categorized into post-training quantization (PTQ) and quantization-\\naware training (QAT), distinguished by whether retraining is involved. PTQ applies quantization directly\\nto a pre-trained model, while QAT includes a retraining stage to mitigate quantization-induced errors.\\nQuantization can target weights, activations, or both. Advanced methods such as GPTQ (Frantar et al.,\\n2023a), AWQ (Lin et al., 2024), and SmoothQuant (Xiao et al., 2023) further enhance quantization for large\\nlanguage models by reducing activation outliers and minimizing calibration errors.\\nPruning.\\nPruning reduces model size and inference latency by eliminating redundant or less important pa-\\nrameters. It can be broadly categorized into unstructured pruning, structured pruning, and semi-structured\\npruning. Unstructured pruning removes individual weights based on certain criteria, such as magnitude.\\nWhile it achieves high sparsity, it is often less hardware-friendly due to irregular sparsity patterns. Struc-\\ntured pruning eliminates entire units such as neurons, channels, or attention heads, leading to more regular\\nsparsity patterns that are easier to accelerate in practice. Semi-structured pruning strikes a balance between\\nthe two, applying constraints such as N:M sparsity, where only a fixed number of weights are retained in\\neach block. This enables efficient execution on specialized hardware. Recent works (e.g., LLM-Pruner, Dep-\\nGraph) (Ma et al., 2023; Fang et al., 2024; 2023; Feng et al., 2024b), and methods based on importance\\nscores and gradient sensitivity (Wang et al., 2021; Zhang et al., 2021; Tuo & Wang, 2025) have significantly\\nimproved the effectiveness and usability of pruning for large models.\\nKnowledge Distillation.\\nKnowledge Distillation (KD) transfers the behavior of a large, well-performing\\nteacher model to a smaller student model by aligning output distributions (e.g., logits or soft labels), interme-\\ndiate representations, or attention patterns. KD approaches can be categorized as black-box or white-box,\\ndepending on whether the student has access only to the teacher’s outputs or to internal states as well.\\nVariants like Self-Instruct KD (Wang et al., 2022b) enable the student to inherit reasoning abilities and\\ngeneralization skills in more flexible settings.\\nA.2\\nOverthinking Example\\nWe provide an example to show the overthinking problem: for the simple question “What is the answer of 1\\nplus 2?”, DeepSeek R1 generates a lengthy response containing 619 tokens, whereas the ideal answer “The\\n30\\nPublished in Transactions on Machine Learning Research (09/2025)\\nanswer is 3.” requires only 17 tokens. This highlights the inefficiency brought by overthinking in current\\nmodels, especially when solving simple problems.\\nAnswer (DeepSeek R1)\\nOkay, so I need to figure out what 1 plus 2 is. Let me start by recalling basic addition. When you\\nadd two numbers together, you’re combining their values. So, if I have 1 apple and someone gives me\\n2 more apples, how many apples do I have in total?\\nLet me count them out. Starting with 1, then adding another 2. So, 1... then 2, 3. Wait, does that\\nmean 1 plus 2 equals 3? Hmm, that seems right. Let me check another way to make sure I’m not\\nmaking a mistake. Maybe using my fingers. Hold up one finger on my left hand for the number 1,\\nthen two fingers on my right hand for the number 2. Now, if I count all the fingers I’m holding up,\\nthat’s 1, 2, 3. Yep, that’s three fingers. So 1 plus 2 must be 3.\\nAlternatively, I can think of the number line. Starting at 1, then moving 2 steps to the right. So\\nfrom 1, moving one step to 2, then another step to 3. That lands me on 3. So that also gives me 3.\\nWait, is there another method to verify? Maybe using objects. If I have 1 book and add 2 more\\nbooks, the total should be 3 books. Let me visualize: one book here, two there. Combine them,\\ncount them all: 1, 2, 3. Yep, three books.\\nI guess all these methods point to the same answer. But just to be thorough, maybe I can use the\\nbasic addition facts I learned. The sum of 1 and 2 is one of the fundamental arithmetic facts. From\\nwhat I remember, 1 + 2 is indeed 3. There’s no carrying over or anything complicated here because\\nboth numbers are single-digit.\\nAnother way to look at it is through set theory. If I have a set with 1 element and another set with\\n2 elements, the union of these sets will have 1 + 2 = 3 elements, provided there’s no overlap. So,\\nassuming the sets are disjoint, the total number of elements is 3.\\nAlso, in terms of equations, if I write it out: 1 + 2 = x. Solving for x, I can subtract 1 from both\\nsides, but that might complicate things. Alternatively, just recognizing that addition is commutative,\\nso 1 + 2 is the same as 2 + 1, which is 3.\\nI think I’ve checked it multiple ways: counting on fingers, using a number line, set theory, and basic\\narithmetic facts. All confirm that 1 plus 2 equals 3. I don’t see any reason to doubt this result. It’s\\nstraightforward and consistent across different methods. So the answer must be 3.\\nAnswer (Expected)\\nThe answer is 3.\\nA.3\\nLatency Comparison of Efficient Reasoning Methods\\nTable 5 summarizes representative efficient reasoning methods on GSM8K across different categories, pro-\\nviding a practical overview of efficient reasoning approaches for users.\\nA.4\\nMetric Formulas\\nA.4.1\\nCarbon Emission\\nCarbon Emission\\n(kgCO2eq)\\n= Energy Consumption\\n(kWh)\\n× Carbon Intensity\\n(gCO2eq/kWh)\\n(1)\\nA.4.2\\nPass@k\\nPass@k = 1 −Etask\\n\"\\x00n−c\\nk\\n\\x01\\n\\x00n\\nk\\n\\x01\\n#\\n(2)\\nwhere n is the number of sampled outputs and c is the number of correct ones.\\n31\\nPublished in Transactions on Machine Learning Research (09/2025)\\nTable 5: Overview of efficient reasoning methods on GSM8K. The speedup ratio is computed mainly through\\nlatency comparison, except for Self-Calibration, where sampling count (S.) is used as a proxy.\\nCategory / Type\\nMethods\\nTraining Scheme\\nAccuracy Base Model\\nSpeedup\\nShorter / Routing\\nSelf-REF\\nSFT (LoRA)\\n81.60%\\nLLaMA3-8B-I\\n1.3 ×\\nSmaller / KD\\nSKIntern\\nDistillation (LoRA)\\n62.50%\\nLLaMA3-8B-I\\n-\\nFaster / Efficient self-consistency Path-Consistency Training-free\\n67.80%\\nLLaMA3-8B-I\\n1.2 ×\\nShorter / SFT\\nCoT-Valve\\nProgressive SFT (LoRA)\\n87.30%\\nLLaMA3.1-8B-I\\n1.7 ×\\nShorter / SFT\\nTokenSkip\\nSFT (LoRA)\\n78.20%\\nLLaMA3.1-8B-I 1.7 - 1.8 ×\\nShorter / SFT\\nTALE-PT\\nSFT (LoRA)\\n78.57%\\nLLaMA3.1-8B-I\\n1.7 ×\\nShorter / Latent reasoning\\nSoftCoT\\nSFT (Freeze FT)\\n81.03%\\nLLaMA3.1-8B-I 4.0 - 5.0 ×\\nShorter / Latent reasoning\\nLightThinker\\nSFT (Full FT)\\n88.25%\\nLLaMA3.1-8B-I up to 1.4 ×\\nShorter / Latent reasoning\\nToken Assorted\\nSFT (Full FT)\\n84.10%\\nLLaMA3.1-8B-I\\n1.2 ×\\nSmaller / KD\\nMix\\nMixed distillation (Full FT & LoRA) 81.40%\\nLLaMA3.1-8B-I\\n-\\nSmaller / KD\\nDLCoT\\nDistillation (Full FT)\\n93.60%\\nLLaMA3.1-8B-I\\n-\\nFaster / Efficient sampling\\nϕ-Decoding\\nTraining-free\\n86.58%\\nLLaMA3.1-8B-I\\n2.8 ×\\nFaster / Efficient self-consistency Self-Calibration\\nSFT (Full FT)\\n80.43%\\nLLaMA3.1-8B-I 16.7 × (S.)\\nA.4.3\\nPass∧k\\nPass ∧k = Etask\\n\"\\x00c\\nk\\n\\x01\\n\\x00n\\nk\\n\\x01\\n#\\n(3)\\nwhere n is the number of sampled outputs and c is the number of correct ones.\\nA.4.4\\nG-Pass@k\\nG-Pass@kτ = Etask\\n\\uf8ee\\n\\uf8f0\\nc\\nX\\nj=⌈τk⌉\\n\\x00c\\nj\\n\\x01\\x00n−c\\nk−j\\n\\x01\\n\\x00n\\nk\\n\\x01\\n\\uf8f9\\n\\uf8fb\\n(4)\\nwhere n is the number of sampled outputs, c is the number of correct ones, and τ is a tolerance threshold\\nthat represents the minimum proportion of correct responses among the k outputs.\\nmG-Pass@kτ = 2\\nk\\nk\\nX\\ni=⌈0.5k⌉+1\\nG-Pass@k i\\nk\\n(5)\\nA.4.5\\nOutcome and Process Efficiency Metric\\nOutcome Efficiency Metric:\\nξO = 1\\nN\\nN\\nX\\ni=1\\nσi\\nˆTi\\nTi\\n(6)\\nwhere N is the number of instances, Ti denotes the total number of tokens generated for instance i, ˆTi is the\\nnumber of tokens until the first correct answer, and σi indicates correctness:\\nσi =\\n(\\n1,\\nif at least one solution is correct\\n0,\\notherwise\\nProcess Efficiency Metric:\\nξP = 1\\nN\\nN\\nX\\ni=1\\nDi\\nTi\\n(7)\\nwhere Di represents tokens contributing to solution diversity, defined as:\\nDi =\\nM\\nX\\nm=1\\nτ m\\ni T m\\ni\\n32\\nPublished in Transactions on Machine Learning Research (09/2025)\\nwhere T m\\ni\\nis the token count of the m-th solution for instance i, and τm\\ni\\ndenotes whether the solution\\nintroduces a new reasoning strategy:\\nτ m\\ni\\n=\\n(\\n1,\\nif solution m is distinct in reasoning\\n0,\\notherwise\\nA.4.6\\nReasoning Boundary (RB)\\nBAcc=K1(t|m) = sup\\nd\\n{d | Acc(t|d, m) = K1}\\n(8)\\nwhere t denotes a specific reasoning task, m represents the evaluated language model, d indicates the difficulty\\nlevel of the task, Acc(t|d, m) is the accuracy of model m on task t with difficulty d, K1 is a predefined\\naccuracy threshold, sup denotes the supremum (least upper bound) over the set of difficulty levels satisfying\\nthe accuracy condition.\\nA.4.7\\nUnderthinking Metric\\nξUT = 1\\nN\\nN\\nX\\ni=1\\n \\n1 −\\nˆTi\\nTi\\n!\\n(9)\\nwhere N is the number of incorrect response instances in the test set, Ti is the total number of tokens in\\nthe i-th incorrect response, ˆTi is the number of tokens from the beginning of the i-th response up to and\\nincluding the first correct thought.\\nA.4.8\\nAccuracy Efficiency Score\\n∆Length = Lengthbaseline −Lengthmodel\\nLengthbaseline\\n,\\n∆Acc = Accmodel −Accbaseline\\nAccbaseline\\nThen, the AES is computed as:\\nAES =\\n(\\nα · ∆Length + β · |∆Acc|,\\nif ∆Acc ≥0\\nα · ∆Length −γ · |∆Acc|,\\nif ∆Acc < 0\\nwhere α > 0, β > 0, and γ > 0 are weighting factors. The default values α = 1, β = 3, and γ = 5 are used\\nto emphasize penalizing accuracy drop more heavily than rewarding accuracy improvement.\\nA.5\\nComplete List of Datasets and Benchmarks\\nA complete list of the datasets and benchmarks used in this area is summarized in Table 6, offering researchers\\nan organized reference for efficient reasoning evaluation.\\n33\\nPublished in Transactions on Machine Learning Research (09/2025)\\nTable 6: Full List of Datasets and Benchmarks.\\nType\\nName\\nTask / Target\\nSource\\nDatasets\\nGSM8K\\nMath\\nHuggingFace Dataset\\nMATH & MATH-500\\nMath\\nHuggingFace Dataset\\nAIME\\nMath\\nHuggingFace Dataset\\nAMC\\nMath\\nHuggingFace Dataset\\nAQuA\\nMath\\nHuggingFace Dataset\\nProntoQA\\nLogical\\nGitHub\\nStrategyQA\\nCommon sense\\nHuggingFace Dataset\\nHotPotQA\\nCommon sense\\nHuggingFace Dataset\\nGame of 24\\nAlgorithmic\\nGitHub\\nBin Packing\\nAlgorithmic\\nGitHub\\nBlocksWorld\\nPlanning\\nHuggingFace Dataset\\nRubik’s Cube\\nPlanning\\nGitHub\\nTrip Plan\\nPlanning\\nGitHub\\nCalendar Plan\\nPlanning\\nGitHub\\nBenchmarks\\nSys2Bench\\nGeneral reasoning\\nGitHub\\nOverthinking Bench\\nOverthinking\\nGitHub\\nBag of Tricks\\nTest-time computation (TTC)\\nGitHub\\nDNA Bench\\nOver-reasoning\\n-\\n34\\n')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## arxiv loader\n",
    "\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "# Supports all arguments of `ArxivAPIWrapper`\n",
    "loader = ArxivLoader(\n",
    "    query=\"2504.10903\",\n",
    "    load_max_docs=2,\n",
    "    # doc_content_chars_max=1000,\n",
    "    # load_all_available_meta=False,\n",
    "    # ...\n",
    ")\n",
    "docs = loader.load()\n",
    "docs\n",
    "# print(type(docs[0]))  # langchain.document.Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89b72eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## wikipedia loader\n",
    "\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "loader = WikipediaLoader(query=\"HUNTER X HUNTER\", load_max_docs=2)\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "376059fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Hunter × Hunter', 'summary': 'Hunter × Hunter (pronounced \"hunter hunter\") is a Japanese manga series written and illustrated by Yoshihiro Togashi. It has been serialized in Shueisha\\'s shōnen manga magazine Weekly Shōnen Jump since March 1998, although the manga has frequently gone on extended hiatuses since 2006. Its chapters have been collected in 38 tankōbon volumes as of September 2024. The story focuses on a young boy named Gon Freecss who discovers that his father, who left him at a young age, is actually a world-renowned Hunter, a licensed professional who specializes in fantastical pursuits such as locating rare or unidentified animal species, treasure hunting, surveying unexplored enclaves, or hunting down lawless individuals. Gon departs on a journey to become a Hunter and eventually find his father. Along the way, Gon meets various other Hunters and encounters the paranormal.\\nHunter × Hunter was adapted into a 62-episode anime television series by Nippon Animation and directed by Kazuhiro Furuhashi, which ran on Fuji Television from October 1999 to March 2001. Three separate original video animations (OVAs) totaling 30 episodes were subsequently produced by Nippon Animation and released in Japan from 2002 to 2004. A second anime television series by Madhouse aired on Nippon Television from October 2011 to September 2014, totaling 148 episodes, with two animated theatrical films released in 2013. There are also numerous audio albums, video games, musicals, and other media based on Hunter × Hunter.\\nThe manga has been licensed for English release in North America by Viz Media since April 2005. Both television series have been also licensed by Viz Media, with the first series having aired on the Funimation Channel in 2009 and the second series broadcast on Adult Swim\\'s Toonami programming block from April 2016 to June 2019.\\nHunter × Hunter has been widely acclaimed and commercially successful, becoming one of the best-selling manga series of all time with over 84 million copies in circulation worldwide by July 2022.', 'source': 'https://en.wikipedia.org/wiki/Hunter_%C3%97_Hunter'}, page_content='Hunter × Hunter (pronounced \"hunter hunter\") is a Japanese manga series written and illustrated by Yoshihiro Togashi. It has been serialized in Shueisha\\'s shōnen manga magazine Weekly Shōnen Jump since March 1998, although the manga has frequently gone on extended hiatuses since 2006. Its chapters have been collected in 38 tankōbon volumes as of September 2024. The story focuses on a young boy named Gon Freecss who discovers that his father, who left him at a young age, is actually a world-renowned Hunter, a licensed professional who specializes in fantastical pursuits such as locating rare or unidentified animal species, treasure hunting, surveying unexplored enclaves, or hunting down lawless individuals. Gon departs on a journey to become a Hunter and eventually find his father. Along the way, Gon meets various other Hunters and encounters the paranormal.\\nHunter × Hunter was adapted into a 62-episode anime television series by Nippon Animation and directed by Kazuhiro Furuhashi, which ran on Fuji Television from October 1999 to March 2001. Three separate original video animations (OVAs) totaling 30 episodes were subsequently produced by Nippon Animation and released in Japan from 2002 to 2004. A second anime television series by Madhouse aired on Nippon Television from October 2011 to September 2014, totaling 148 episodes, with two animated theatrical films released in 2013. There are also numerous audio albums, video games, musicals, and other media based on Hunter × Hunter.\\nThe manga has been licensed for English release in North America by Viz Media since April 2005. Both television series have been also licensed by Viz Media, with the first series having aired on the Funimation Channel in 2009 and the second series broadcast on Adult Swim\\'s Toonami programming block from April 2016 to June 2019.\\nHunter × Hunter has been widely acclaimed and commercially successful, becoming one of the best-selling manga series of all time with over 84 million copies in circulation worldwide by July 2022.\\n\\n\\n== Synopsis ==\\n\\n\\n=== Setting ===\\nIn the world of Hunter × Hunter, Hunters (ハンター, Hantā) are licensed, elite members of humanity who are capable of tracking down secret treasures, rare beasts, or even other individuals, and can also access locations that regulars cannot. To obtain a license, one must pass the rigorous annual Hunter Examination run by the Hunter Association, which has a success rate of less than one in a hundred-thousand. A Hunter may be awarded up to three stars: a single star for making \"remarkable achievements in a particular field\"; they may then be upgraded to two stars for \"holding an official position\" and mentoring another Hunter up to single star level; and finally upgraded to three stars for \"remarkable achievements in multiple fields\".\\nNen (念) is the ability to control one\\'s own life energy or aura, which is constantly emitted from them, knowingly or not. There are four basic Nen techniques: Ten (纏) maintains the aura in the body, strengthening it for defense; Zetsu (絕) shuts the aura flow off, useful for concealing one\\'s presence and relieving fatigue; Ren (練) enables a user to produce more Nen; and Hatsu (發) is a person\\'s specific use of Nen. Nen users are classified into six types based on their Hatsu abilities; Enhancers (強化系, Kyōkakei) strengthen and reinforce their natural physical abilities; Emitters (放出系, Hōshutsukei) project aura out of their bodies; Manipulators (操作系, Sōsakei) control objects or living things; Transmuters (変化系, Henkakei) change the type or properties of their aura; Conjurers (具現化系, Gugenkakei) create objects out of their aura; and Specialists (特質系, Tokushitsukei) have unique abilities that do not fall into the previous categories. A Nen user can enter into a Contract (誓約, Seiyaku) where, by pledging to follow certain Limitations (制約, Seiyaku), their abilities are strengthened in relation to how strict they are. An example of this is Kurapika who, in order to have an unbreakable chain'),\n",
       " Document(metadata={'title': 'Hunter × Hunter (2011 TV series)', 'summary': 'Hunter × Hunter is a Japanese anime television series, based on Yoshihiro Togashi\\'s manga series Hunter × Hunter, aired from 2011 to 2014. The story begins with a young boy named Gon Freecss, who one day discovers that the father who he thought was dead is, in fact, alive and well. He learns that his father, Ging, is a legendary \"Hunter\", an individual who has proven themselves an elite member of humanity. Despite the fact that Ging left his son with his relatives in order to pursue his own dreams, Gon becomes determined to follow in his father\\'s footsteps, pass the rigorous \"Hunter Examination\", and eventually find his father to become a Hunter in his own right.\\nThis second anime television series adaptation of Hunter × Hunter was announced on July 24, 2011. It is a complete reboot starting from the beginning of the original manga, with no connection to the first anime television series from 1999. Produced by Nippon Television, VAP, Shueisha, and Madhouse, the series is directed by Hiroshi Kōjina, with Atsushi Maekawa and Tsutomu Kamishiro handling series composition, Takahiro Yoshimatsu designing the characters and Yoshihisa Hirano composing the music. Instead of having the old cast reprise their roles for the new adaptation, the series features an entirely new cast to voice the characters. The new series premiered airing weekly on Nippon Television and the nationwide Nippon News Network from October 2, 2011.  The series started to be collected in both DVD and Blu-ray format on January 25, 2012. Viz Media has licensed the anime for a DVD/Blu-ray release in North America with an English dub. On television, the series began airing on Adult Swim\\'s Toonami programming block on April 17, 2016, and ended on June 23, 2019.\\nThe anime series\\' opening theme is alternated between the song \"Departure!\" and an alternate version titled \"Departure! -Second Version-\" both sung by Galneryus\\' vocalist Masatoshi Ono. Five pieces of music were used as the ending theme; \"Just Awake\" by the Japanese band Fear, and Loathing in Las Vegas in episodes 1 to 26, \"Hunting for Your Dream\" by Galneryus in episodes 27 to 58, \"Reason\" sung by Japanese duo Yuzu in episodes 59 to 75, \"Nagareboshi Kirari\" also sung by Yuzu from episode 76 to 98, which was originally from the anime film adaptation, Hunter × Hunter: Phantom Rouge, and \"Hyōri Ittai\" by Yuzu featuring Hyadain from episode 99 to 146, which was also used in the film Hunter × Hunter: The Last Mission. The background music and soundtrack for the series was composed by Yoshihisa Hirano.', 'source': 'https://en.wikipedia.org/wiki/Hunter_%C3%97_Hunter_(2011_TV_series)'}, page_content='Hunter × Hunter is a Japanese anime television series, based on Yoshihiro Togashi\\'s manga series Hunter × Hunter, aired from 2011 to 2014. The story begins with a young boy named Gon Freecss, who one day discovers that the father who he thought was dead is, in fact, alive and well. He learns that his father, Ging, is a legendary \"Hunter\", an individual who has proven themselves an elite member of humanity. Despite the fact that Ging left his son with his relatives in order to pursue his own dreams, Gon becomes determined to follow in his father\\'s footsteps, pass the rigorous \"Hunter Examination\", and eventually find his father to become a Hunter in his own right.\\nThis second anime television series adaptation of Hunter × Hunter was announced on July 24, 2011. It is a complete reboot starting from the beginning of the original manga, with no connection to the first anime television series from 1999. Produced by Nippon Television, VAP, Shueisha, and Madhouse, the series is directed by Hiroshi Kōjina, with Atsushi Maekawa and Tsutomu Kamishiro handling series composition, Takahiro Yoshimatsu designing the characters and Yoshihisa Hirano composing the music. Instead of having the old cast reprise their roles for the new adaptation, the series features an entirely new cast to voice the characters. The new series premiered airing weekly on Nippon Television and the nationwide Nippon News Network from October 2, 2011.  The series started to be collected in both DVD and Blu-ray format on January 25, 2012. Viz Media has licensed the anime for a DVD/Blu-ray release in North America with an English dub. On television, the series began airing on Adult Swim\\'s Toonami programming block on April 17, 2016, and ended on June 23, 2019.\\nThe anime series\\' opening theme is alternated between the song \"Departure!\" and an alternate version titled \"Departure! -Second Version-\" both sung by Galneryus\\' vocalist Masatoshi Ono. Five pieces of music were used as the ending theme; \"Just Awake\" by the Japanese band Fear, and Loathing in Las Vegas in episodes 1 to 26, \"Hunting for Your Dream\" by Galneryus in episodes 27 to 58, \"Reason\" sung by Japanese duo Yuzu in episodes 59 to 75, \"Nagareboshi Kirari\" also sung by Yuzu from episode 76 to 98, which was originally from the anime film adaptation, Hunter × Hunter: Phantom Rouge, and \"Hyōri Ittai\" by Yuzu featuring Hyadain from episode 99 to 146, which was also used in the film Hunter × Hunter: The Last Mission. The background music and soundtrack for the series was composed by Yoshihisa Hirano.\\n\\n\\n== Series overview ==\\n\\n\\n== Episodes ==\\n\\n\\n=== Hunter Exam arc (2011–12) ===\\n\\n\\n=== Heavens Arena arc (2012) ===\\n\\n\\n=== Phantom Troupe arc (2012) ===\\n\\n\\n=== Greed Island arc (2012–13) ===\\n\\n\\n=== Chimera Ant arc (2013–14) ===\\n\\n\\n=== Election arc (2014) ===\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nHunter × Hunter at Nippon Television\\nHunter x Hunter at Crunchyroll\\nHunter x Hunter (anime) at Anime News Network\\'s encyclopedia\\nHunter × Hunter at IMDb')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
